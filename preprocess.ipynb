{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bbc4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPTIONS_PATH = \"Flickr8k/Flickr8k.token.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c782de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRawImageWith5Captions():\n",
    "    items = dict()\n",
    "    with open(CAPTIONS_PATH, \"r\") as f:\n",
    "        raw_data = f.read()\n",
    "    f.close()\n",
    "    lines = raw_data.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if len(line) > 0:\n",
    "            img_path, caption = line.split(\"\\t\")\n",
    "            img_path = img_path.split(\"#\")[0]\n",
    "            if img_path not in items:\n",
    "                items[img_path] = []\n",
    "            caption = caption.lower()\n",
    "            caption = caption.strip(\" .\")\n",
    "            # caption = '<START> ' + caption + ' <END>'\n",
    "            items[img_path].append(caption)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad39b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = getRawImageWith5Captions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70db3d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_img_paths = \"Flickr8k\\Flickr_8k.trainImages.txt\"\n",
    "all_test_img_paths = \"Flickr8k\\Flickr_8k.testImages.txt\"\n",
    "all_val_img_paths = \"Flickr8k\\Flickr_8k.devImages.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd1b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(all_train_img_paths, 'r') as f:\n",
    "    all_train_img = f.read().split(\"\\n\")\n",
    "\n",
    "with open(all_test_img_paths, 'r') as f:\n",
    "    all_test_img = f.read().split(\"\\n\")\n",
    "\n",
    "with open(all_val_img_paths, 'r') as f:\n",
    "    all_val_img = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c90351ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_data = dict()\n",
    "for img in all_train_img:\n",
    "    if img in raw_data:\n",
    "        train_raw_data[img] = raw_data[img]\n",
    "\n",
    "test_raw_data = dict()\n",
    "for img in all_test_img:\n",
    "    if img in raw_data:\n",
    "        test_raw_data[img] = raw_data[img]\n",
    "\n",
    "val_raw_data = dict()\n",
    "for img in all_val_img:\n",
    "    if img in raw_data:\n",
    "        val_raw_data[img] = raw_data[img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "621c4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = []\n",
    "for image in train_raw_data:\n",
    "    for caption in train_raw_data[image]:\n",
    "        train_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c53e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "for caption in train_captions:\n",
    "    for word in caption.split(' '):\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "min_freq = 5\n",
    "special_tokens = [\"<PAD>\", \"<START>\", \"<END>\", \"<UNK>\"]\n",
    "vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "decode_vocab = {idx: token for idx, token in enumerate(special_tokens)}\n",
    "idx = 4\n",
    "for word, freq in word_freq.items():\n",
    "    if freq >= min_freq and word not in special_tokens:\n",
    "        vocab[word] = idx\n",
    "        decode_vocab[idx] = word\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1fb3f8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 0, 0]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_len = 5\n",
    "test = [1,2]\n",
    "if len(test) >= test_len:\n",
    "    test = test[:test_len - 1]\n",
    "    test.append(2)\n",
    "elif len(test) < test_len:\n",
    "    test.append(2)\n",
    "    while len(test) < test_len:\n",
    "        test.append(0)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6a2fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 15\n",
    "def make_sequence(raw_data):\n",
    "    sequences = {}\n",
    "    for image, captions in raw_data.items():\n",
    "        sequences[image] = []\n",
    "        for caption in captions:\n",
    "            sequence = [1]\n",
    "            for word in caption.split(' '):\n",
    "                if word not in vocab:\n",
    "                    word = \"<UNK>\"\n",
    "                sequence.append(vocab[word])\n",
    "            if len(sequence) >= max_len:\n",
    "                sequence = sequence[:max_len - 1]\n",
    "                sequence.append(2)\n",
    "            elif len(sequence) < max_len:\n",
    "                sequence.append(2)\n",
    "                while(len(sequence) < max_len):\n",
    "                    sequence.append(0)\n",
    "            sequences[image].append(sequence)\n",
    "    return sequences\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0981923",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = make_sequence(train_raw_data)\n",
    "val_sequences = make_sequence(val_raw_data)\n",
    "test_sequences = make_sequence(test_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "120c5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sequence):\n",
    "    result = ''\n",
    "    for word in sequence:\n",
    "        if decode_vocab[word] not in [token for token in special_tokens if token != \"<UNK>\"]:\n",
    "            result += decode_vocab[word] + ' '\n",
    "        else:\n",
    "            continue\n",
    "    result.strip()\n",
    "    \n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4728ed1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a boy is creating large splashes whilst swimming in the ocean \n",
      "a boy with black hair and dark <UNK> is swimming in murky water \n",
      "a child splashes in a lake \n",
      "a little boy jumped into the water and made a big splash \n",
      "a young boy falling into a body of water \n"
     ]
    }
   ],
   "source": [
    "for seq in train_sequences['3457856049_2de173e818.jpg']:\n",
    "    print(decode(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff42b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "if not os.path.exists('Processed Data'):\n",
    "    os.mkdir('Processed Data')\n",
    "# Lưu vocab\n",
    "with open('Processed Data/vocab.json', 'w') as f:\n",
    "    json.dump(vocab, f)\n",
    "\n",
    "with open('Processed Data/decode_vocab.json', 'w') as f:\n",
    "    json.dump(decode_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d2b4212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu sequences đã xử lý\n",
    "with open('Processed Data/train_sequences.pkl', 'wb') as f:\n",
    "    pickle.dump(train_sequences, f)\n",
    "\n",
    "with open('Processed Data/val_sequences.pkl', 'wb') as f:\n",
    "    pickle.dump(val_sequences, f)\n",
    "\n",
    "with open('Processed Data/test_sequences.pkl', 'wb') as f:\n",
    "    pickle.dump(test_sequences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1cf5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'vocab_size': len(vocab),\n",
    "    'max_length': max_len,\n",
    "    'min_freq': min_freq,\n",
    "    'train_size': len(train_sequences),\n",
    "    'val_size': len(val_sequences),\n",
    "    'test_size': len(test_sequences)\n",
    "}\n",
    "\n",
    "with open('Processed Data/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

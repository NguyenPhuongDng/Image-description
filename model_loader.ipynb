{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fine_Tune.util import load_model_chunks, save_model_chunks # import cái này\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms as T\n",
    "# from Data_loader import FlickrDataset,get_data_loader\n",
    "from CNN import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        # chuyển đổi chiều của encoder và decoder về cùng chiều với attention để tìm đặc trưng quan trọng đồng nhất\n",
    "        self.attention_dim = attention_dim\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.full_att = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out) # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden) # (batch_size, attention_dim)\n",
    "        att = torch.tanh(att1 + att2.unsqueeze(1)) # (batch_size, num_pixels, attention_dim)\n",
    "\n",
    "        attention_scores = self.full_att(att) # (batch_size, num_pixels, 1)\n",
    "        attention_scores = attention_scores.squeeze(2) # (batch_size, num_pixels)\n",
    "\n",
    "        # hệ số của trọng số chú ý\n",
    "        alpha = F.softmax(attention_scores, dim=1) # (batch_size, num_pixels) pixel càng quan trọng giá trị càng cao\n",
    "        \n",
    "        # trọng số chú ý\n",
    "        attention_weights = encoder_out * alpha.unsqueeze(2) # (batch_size, num_pixels, encoder_out)\n",
    "        attention_weights = attention_weights.sum(dim=1) # (batch_size, encoder_out)\n",
    "\n",
    "        return alpha, attention_weights\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        #save the model param\n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention_dim = attention_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n",
    "\n",
    "        \n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
    "        \n",
    "        \n",
    "        self.fcn = nn.Linear(decoder_dim,vocab_size)\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \n",
    "        embeds = self.embedding(captions)\n",
    "        \n",
    "        h, c = self.init_hidden_state(features)  \n",
    "       \n",
    "        seq_length = len(captions[0])-1 \n",
    "        batch_size = captions.size(0)\n",
    "        num_features = features.size(1)\n",
    "        \n",
    "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n",
    "                \n",
    "        for s in range(seq_length):\n",
    "            alpha,context = self.attention(features, h)\n",
    "            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "                    \n",
    "            output = self.fcn(self.drop(h))\n",
    "            \n",
    "            preds[:,s] = output\n",
    "            alphas[:,s] = alpha  \n",
    "        \n",
    "        return preds, alphas\n",
    "    \n",
    "    def generate_caption(self,features,max_len=20,vocab=None):\n",
    "        \n",
    "        \n",
    "        \n",
    "        batch_size = features.size(0)\n",
    "        h, c = self.init_hidden_state(features)  \n",
    "        \n",
    "        alphas = []\n",
    "        \n",
    "        #starting input\n",
    "        word = torch.tensor(vocab.stoi['<SOS>']).view(1,-1).to(device)\n",
    "        embeds = self.embedding(word)\n",
    "\n",
    "        \n",
    "        captions = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            alpha,context = self.attention(features, h)\n",
    "            \n",
    "            \n",
    "            #store the apla score\n",
    "            alphas.append(alpha.cpu().detach().numpy())\n",
    "            \n",
    "            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "            output = self.fcn(self.drop(h))\n",
    "            output = output.view(batch_size,-1)\n",
    "        \n",
    "            \n",
    "            #select the word with most val\n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "            \n",
    "            #save the generated word\n",
    "            captions.append(predicted_word_idx.item())\n",
    "            \n",
    "            #end if <EOS detected>\n",
    "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
    "                break\n",
    "            \n",
    "            #send generated word as the next caption\n",
    "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "        \n",
    "        #covert the vocab idx to words and return sentence\n",
    "        return [vocab.itos[idx] for idx in captions],alphas\n",
    "    \n",
    "    \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderCNN()\n",
    "        self.decoder = DecoderRNN(\n",
    "            embed_size=embed_size,\n",
    "            vocab_size = len(dataset.vocab),\n",
    "            attention_dim=attention_dim,\n",
    "            encoder_dim=encoder_dim,\n",
    "            decoder_dim=decoder_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)        \n",
    "        outputs = self.decoder(features, captions)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anh\\AppData\\Local\\Temp\\ipykernel_36320\\2998504169.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(\"image_caption_model_4.pth\")\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(\"image_caption_model_4.pth\")\n",
    "save_model_chunks(\n",
    "    model=data,\n",
    "    folder_path=\"BaseModel\",\n",
    "    name=\"model4\",\n",
    "    max_chunk_size=95*1024*1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_model_chunks(\n",
    "    folder_path=\"BaseModel\",\n",
    "    name=\"model4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['num_epochs', 'embed_size', 'vocab_size', 'attention_dim', 'encoder_dim', 'decoder_dim', 'state_dict'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format lại code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import math\n",
    "import torch.multiprocessing as mp\n",
    "from transformers import BertTokenizerFast\n",
    "import pickle\n",
    "# Evaluate\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# from nltk.translate.meteor_score import meteor_score\n",
    "# from rogue import Rogue\n",
    "# from pycocoevalcap.cider.cider import Cider\n",
    "# from pycocoevalcap.spice.spice import Spice\n",
    "#end\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "detokenize = tokenizer.convert_ids_to_tokens\n",
    "batch_detokenize = tokenizer.batch_decode\n",
    "BATCH_SIZE = 64 \n",
    "#64 : 3.1GB VRAM b16 (3.0 Dedicated | 0.1 Shared)\n",
    "device = 'cuda'\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Dùng cho ImageNet\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loading 10.01 % \n",
      "Image loading 20.02 % \n",
      "Image loading 30.03 % \n",
      "Image loading 40.04 % \n",
      "Image loading 50.06 % \n",
      "Image loading 60.07 % \n",
      "Image loading 70.08 % \n",
      "Image loading 80.09 % \n",
      "Image loading 90.10 % \n",
      "Image loading 100 % \n"
     ]
    }
   ],
   "source": [
    "def load_image(image_folder_path: str): # DataLoader trên Jupyternotebook ko xử lý đa luồng đc nên đành phải load hết vô\n",
    "    result = {}\n",
    "    file_counts = len(os.listdir(image_folder_path))\n",
    "    progress = 0\n",
    "    last_log = 0\n",
    "    count = 0\n",
    "    for file_name in os.listdir(image_folder_path):\n",
    "        file_path = os.path.join(image_folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            id = os.path.splitext(os.path.basename(file_name))[0]\n",
    "            if id in result: continue\n",
    "            with Image.open(file_path).convert(\"RGB\") as img:\n",
    "                image = image_transforms(img)\n",
    "                result[id] = torch.Tensor(image)\n",
    "            count += 1\n",
    "            progress = count / file_counts * 100\n",
    "            if progress - last_log >= 10:\n",
    "                last_log = progress\n",
    "                print(f\"Image loading {progress:.2f} % \")\n",
    "    if last_log != 100:\n",
    "        print(f\"Image loading 100 % \")\n",
    "    return result\n",
    "def process_data(image_dict: dict[str, torch.Tensor], processed_data_path: str):\n",
    "    result: list[tuple[torch.Tensor, torch.Tensor]] = []\n",
    "    with open(processed_data_path, 'rb') as file:\n",
    "        processed_data = pickle.load(file)\n",
    "    for image_id, caption in processed_data:\n",
    "        caption = torch.tensor(caption)\n",
    "        image = image_dict[image_id]\n",
    "        result.append((image, caption))\n",
    "    return result\n",
    "def get_train_test_loader(batch_size: int, n_wokers: int = 2):\n",
    "    image_path = \"../Flickr8k/Flicker8k_Dataset\"\n",
    "    train_path = \"../Data_bert/train_set_bert.pkl\"\n",
    "    test_path = \"../Data_bert/test_set_bert.pkl\"\n",
    "    image_dict: dict[str, torch.Tensor] = load_image(image_path)\n",
    "    train_data = process_data(image_dict, train_path)\n",
    "    test_data = process_data(image_dict, test_path)\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, num_workers=n_wokers, shuffle=True)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, num_workers=n_wokers, shuffle=False)\n",
    "    return trainloader, testloader\n",
    "trainloader, testloader = get_train_test_loader(BATCH_SIZE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, captions in trainloader:\n",
    "#     for i in range(min(4, captions.shape[0])):\n",
    "#         print(images[i][:,100,100])\n",
    "#         print(detokenize(captions[i]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, output_size: int):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.inception_model = models.inception_v3(pretrained=True)\n",
    "        #self.inception_model.fc = torch.nn.Identity()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(1000, output_size)\n",
    "        for name, param in self.inception_model.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    def forward(self, images: torch.Tensor):\n",
    "        features = self.inception_model(images) #[1, 2048]\n",
    "        if isinstance(features, tuple):  # Nếu là tuple\n",
    "            features = features[0] \n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "        features = self.fc(features)\n",
    "        return features\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, hidden_size, input_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_hidden = False\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, hidden_state: tuple[torch.Tensor, torch.Tensor] = None): \n",
    "        # Constant : seq = 1\n",
    "        # features : [bsz, img_sz]\n",
    "        # captions : [bsz, seq]\n",
    "        # hidden : [num_layers, bsz, hidden]\n",
    "        embeddings = self.embed(captions) # [bsz, seq, embed]\n",
    "        features = features.unsqueeze(1).expand(-1, embeddings.shape[1], -1) # [bsz, seq, embed]\n",
    "        combined = torch.cat((features, embeddings), dim=2) # [bsz, seq, img_sz + embed]\n",
    "        # hidden_state : [num_layers, seq, hid] * 2\n",
    "        if self.use_hidden:\n",
    "            output, hidden_state = self.lstm(combined, hidden_state)\n",
    "        else:\n",
    "            output, hidden_state = self.lstm(combined)\n",
    "        # output : [bsz, seq, vocab_size]\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden_state\n",
    "class ImageToTextModel(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
    "        super(ImageToTextModel, self).__init__()\n",
    "        self.encoder: EncoderCNN = encoder\n",
    "        self.decoder: DecoderRNN = decoder\n",
    "    def forward(self, images: torch.Tensor, captions: torch.Tensor):\n",
    "        # Constant : SEQ_LENGTH = 46, seq = 1\n",
    "        # images: [bsz, 3, raw_image_width, raw_image_height]\n",
    "        # captions: [bsz, SEQ_LENGTH]\n",
    "        bsz = images.shape[0]\n",
    "        if self.decoder.use_hidden:\n",
    "            hidden_state: tuple[torch.Tensor, torch.Tensor] = None\n",
    "        features = self.encoder(images)\n",
    "        # features: [bsz, img_sz]\n",
    "        seq_predicted = []\n",
    "        seq_predicted.append(torch.zeros((bsz, self.decoder.vocab_size), dtype=torch.float32).unsqueeze(1).to(device))\n",
    "        # seq_predicted : [predict_length, seq, vocab]\n",
    "        decoder_input = captions[:, 0].unsqueeze(1)\n",
    "        # decoder_input : [bsz, seq]\n",
    "        seq_length = captions.shape[1]\n",
    "        for di in range(1, seq_length):\n",
    "            if self.decoder.use_hidden:\n",
    "                output_decoder, hidden_state = self.decoder(features, decoder_input, hidden_state)\n",
    "            else:\n",
    "                output_decoder, hidden_state = self.decoder(features, decoder_input)\n",
    "            # ouput_decoder: [bsz, seq, vocab]\n",
    "            # hidden_state: [num_layers, bsz, hidden_size] * 2\n",
    "            decoder_input = captions[:, di].unsqueeze(1)\n",
    "            # decoder_input : [bsz, seq]\n",
    "            seq_predicted.append(output_decoder)\n",
    "        return torch.cat(seq_predicted, dim=1)\n",
    "    def predict(self, images: torch.Tensor, captions: torch.Tensor, predict_length: int):\n",
    "        bsz = images.shape[0]\n",
    "        hidden_state: tuple[torch.Tensor, torch.Tensor] = None\n",
    "        features = self.encoder(images)\n",
    "        seq_predicted = []\n",
    "        seq_predicted.append(torch.zeros((bsz, self.decoder.vocab_size), dtype=torch.float32).unsqueeze(1).to(device))\n",
    "        decoder_input = captions\n",
    "        for _ in range(predict_length-1):\n",
    "            output_decoder, hidden_state = self.decoder(features, decoder_input, hidden_state)\n",
    "            seq_predicted.append(output_decoder)\n",
    "            decoder_input = output_decoder.argmax(2)\n",
    "        return torch.cat(seq_predicted, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model : ImageToTextModel, dataloader : DataLoader, lossf : callable, optimizer : torch.optim.Optimizer, mixed: bool, device: str):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    count = 0\n",
    "    for images, captions in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        images: torch.Tensor = images.to(device)\n",
    "        captions: torch.Tensor = captions.to(device)\n",
    "        if mixed:\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                outputs: torch.Tensor = model(images, captions)\n",
    "                loss: torch.Tensor = lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        else:\n",
    "            outputs: torch.Tensor = model(images, captions)\n",
    "            loss: torch.Tensor = lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        count += 1 \n",
    "    return epoch_loss / count\n",
    "def compute_metrics(predict: torch.Tensor, caption: torch.Tensor): # Quên chưa xóa token đầu ...\n",
    "    # Accuracy\n",
    "    is_correct = 1 if predict[1] == caption[1] else 0\n",
    "    caption: list[str] = detokenize(caption)\n",
    "    predict: list[str] = detokenize(predict)\n",
    "    while (caption[-1] == \"[PAD]\"):\n",
    "        caption.pop()\n",
    "    while (predict[-1] == \"[PAD]\"):\n",
    "        predict.pop()\n",
    "    # Bleu\n",
    "    reference = [caption]\n",
    "    # print(caption, predict)\n",
    "    bleu_1 = sentence_bleu(reference, predict, weights=(1, 0, 0, 0))\n",
    "    bleu_2 = sentence_bleu(reference, predict, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu_3 = sentence_bleu(reference, predict, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu_4 = sentence_bleu(reference, predict)\n",
    "    # Rogue\n",
    "    # rogue = Rogue()\n",
    "    # scores = rogue.get_scores(' '.join(predicts), ' '.join(captions))\n",
    "    # Meteor\n",
    "    # meteor_score_ = meteor_score([' '.join(captions)], ' '.join(predicts))\n",
    "    # Cider, spice\n",
    "    # cider_scorer = Cider()\n",
    "    # spice_scorer = Spice()\n",
    "    # cider_score, _ = cider_scorer.compute_score({0 : [' '.join(caption)]}, {0 : [' '.join(predict)]})\n",
    "    # spice_score, _ = spice_scorer.compute_score({0 : [' '.join(captions)]}, {0 : [' '.join(predicts)]})\n",
    "    return {\n",
    "        \"accuracy\" : is_correct,\n",
    "        \"bleu_1\" : bleu_1,\n",
    "        \"bleu_2\" : bleu_2,\n",
    "        \"bleu_3\" : bleu_3,\n",
    "        \"bleu_4\" : bleu_4,\n",
    "        # \"rogue-1\" : scores[\"rogue-1\"][\"f\"],\n",
    "        # \"rogue-2\" : scores[\"rogue-2\"][\"f\"],\n",
    "        # \"rogue-l\" : scores[\"rogue-l\"][\"f\"],\n",
    "        # \"meteor\" : meteor_score_,\n",
    "        # \"cider\" : cider_score,\n",
    "        # \"spice\" : spice_score\n",
    "    }\n",
    "def test(model : ImageToTextModel, dataloader : DataLoader, lossf : callable, mixed: bool, device: str):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    total_count = 0\n",
    "    epoch_metrics = {'loss' : 0}\n",
    "    input = torch.tensor([101]).to(device)\n",
    "    for images, captions in dataloader:\n",
    "        images: torch.Tensor = images.to(device)\n",
    "        captions: torch.Tensor = captions.to(device)\n",
    "        bsz = images.shape[0]\n",
    "        inputs = input.unsqueeze(1).expand((bsz, 1))\n",
    "        if mixed:\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                outputs: torch.Tensor = model.predict(images, inputs, captions.shape[1])\n",
    "                loss: torch.Tensor= lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        else:\n",
    "            outputs: torch.Tensor  = model.predict(images, inputs, captions.shape[1])\n",
    "            loss: torch.Tensor= lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        predicts = outputs.argmax(2)\n",
    "        count += 1\n",
    "        total_count += bsz\n",
    "        epoch_metrics['loss'] += loss.item()\n",
    "        for i in range(bsz):\n",
    "            metrics = compute_metrics(predicts[i], captions[i])\n",
    "            for key in metrics:\n",
    "                if key not in epoch_metrics:\n",
    "                    epoch_metrics[key] = metrics[key]\n",
    "                else:\n",
    "                    epoch_metrics[key] += metrics[key]\n",
    "    for key in epoch_metrics:\n",
    "        if key in ['loss']:\n",
    "            epoch_metrics[key] /= count\n",
    "        else:\n",
    "            epoch_metrics[key] /= total_count\n",
    "    return epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint\n",
      "Load optimizer, schedulers\n",
      "Start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 5.4592 | Test loss : 7.5487 | Train time : 3.08 s | Lr : 0.00250000\n",
      "{'loss': 7.548660755157471, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:06<00:58,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 5.0287 | Test loss : 7.5709 | Train time : 2.95 s | Lr : 0.00237500\n",
      "{'loss': 7.570857048034668, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:12<00:51,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.7354 | Test loss : 7.6682 | Train time : 3.22 s | Lr : 0.00225625\n",
      "{'loss': 7.668242454528809, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:19<00:46,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.5769 | Test loss : 7.7868 | Train time : 3.05 s | Lr : 0.00214344\n",
      "{'loss': 7.786815643310547, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:26<00:39,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.5028 | Test loss : 7.9144 | Train time : 3.16 s | Lr : 0.00203627\n",
      "{'loss': 7.914384365081787, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:33<00:33,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.3936 | Test loss : 8.0518 | Train time : 3.03 s | Lr : 0.00193445\n",
      "{'loss': 8.051778793334961, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:39<00:26,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.4135 | Test loss : 8.1938 | Train time : 3.01 s | Lr : 0.00183773\n",
      "{'loss': 8.193782806396484, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:46<00:19,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.4496 | Test loss : 8.3104 | Train time : 3.36 s | Lr : 0.00174584\n",
      "{'loss': 8.3104248046875, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:53<00:13,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.3580 | Test loss : 8.4168 | Train time : 3.23 s | Lr : 0.00165855\n",
      "{'loss': 8.416780471801758, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:05<00:00,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.3454 | Test loss : 8.5201 | Train time : 3.16 s | Lr : 0.00157562\n",
      "{'loss': 8.520052909851074, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "image_size = 256\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "encoder = EncoderCNN(\n",
    "    output_size=image_size\n",
    ")\n",
    "decoder = DecoderRNN(\n",
    "    embed_size=embed_size,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=hidden_size,\n",
    "    input_size=image_size,\n",
    "    num_layers=1\n",
    ")\n",
    "decoder.use_hidden = True\n",
    "image_to_text_model = ImageToTextModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder\n",
    ")\n",
    "loss_func= nn.CrossEntropyLoss(ignore_index=0)\n",
    "model_path, train_log = util.train_eval(\n",
    "    trainloader=trainloader, # train data loader\n",
    "    testloader=testloader, # test data loader\n",
    "    model=image_to_text_model, # model\n",
    "    train_func=train, # hàm train\n",
    "    test_func=test, # hàm test\n",
    "    lossf=loss_func, # hàm loss\n",
    "    end_epoch=50, # số epoch kết thúc train (tăng lên khi train từ checkpoint nhá, ko phải số epoch nó chạy code mà số epoch nó sẽ dừng)\n",
    "    lr=2.5e-3, # lr sau khi warmup\n",
    "    gamma=0.95, # lr * gamma ^ epoch sau warmup\n",
    "    log_step=1, # in ra console mỗi k epochs\n",
    "    warmup_nepochs=10, # số epoch chạy warmup scheduler\n",
    "    warmup_lr=1e-3, # lr ban đầu của warmup\n",
    "    warmup_gamma=1.1, # lr * gamma ^ epoch khi chạy warmup\n",
    "    save_weight=True, # lưu trọng số mô hình\n",
    "    save_full=True, # lưu optimizer, scheduler để train\n",
    "    save_each=1, # luu mỗi k epoch\n",
    "    mixed_train = True, # sử dụng bf16 cho train\n",
    "    mixed_eval = False, # sử dùng bf16 cho eval\n",
    "    load_checkpoint = True, # load checkpoint cũ ko\n",
    "    load_optimizer = True, # load optimizer cũ ko (sau khi load checkpoint)\n",
    "    checkpoint_path = \"checkpoint/i256_e256_h256_model/2024-12-12_20-37-10\", # folder path của checkpoint\n",
    "    save_path=\"checkpoint/i256_e256_h_256_model\", # folder path lưu mô hình\n",
    "    optimizer_type=torch.optim.Adam, # loại optimizer sử dụng\n",
    "    device=device, # cpu hay cuda\n",
    "    metadata_extra={ # tham số lưu thêm vào metada khi lưu mô hình\n",
    "        \"batch_size\" : BATCH_SIZE,\n",
    "        \"dataset_name\" : \"Flickr8k_sample10\",\n",
    "        \"use_hidden\" : decoder.use_hidden\n",
    "    },\n",
    "    log_metric=True # in các metrics ra cốnle\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format lại code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import math\n",
    "import torch.multiprocessing as mp\n",
    "from transformers import BertTokenizerFast\n",
    "import pickle\n",
    "# Evaluate\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# from nltk.translate.meteor_score import meteor_score\n",
    "# from rogue import Rogue\n",
    "# from pycocoevalcap.cider.cider import Cider\n",
    "# from pycocoevalcap.spice.spice import Spice\n",
    "#end\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "detokenize = tokenizer.convert_ids_to_tokens\n",
    "batch_detokenize = tokenizer.batch_decode\n",
    "BATCH_SIZE = 64 \n",
    "#64 : 3.1GB VRAM b16 (3.0 Dedicated | 0.1 Shared)\n",
    "device = 'cuda'\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Dùng cho ImageNet\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_folder_path: str): # DataLoader trên Jupyternotebook ko xử lý đa luồng đc nên đành phải load hết vô\n",
    "    result = {}\n",
    "    file_counts = len(os.listdir(image_folder_path))\n",
    "    progress = 0\n",
    "    last_log = 0\n",
    "    count = 0\n",
    "    for file_name in os.listdir(image_folder_path):\n",
    "        file_path = os.path.join(image_folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            id = os.path.splitext(os.path.basename(file_name))[0]\n",
    "            if id in result: continue\n",
    "            with Image.open(file_path).convert(\"RGB\") as img:\n",
    "                image = image_transforms(img)\n",
    "                result[id] = torch.Tensor(image)\n",
    "            count += 1\n",
    "            progress = count / file_counts * 100\n",
    "            if progress - last_log >= 10:\n",
    "                last_log = progress\n",
    "                print(f\"Image loading {progress:.2f} % \")\n",
    "    if last_log != 100:\n",
    "        print(f\"Image loading 100 % \")\n",
    "    return result\n",
    "def process_data(image_dict: dict[str, torch.Tensor], processed_data_path: str):\n",
    "    result: list[tuple[torch.Tensor, torch.Tensor]] = []\n",
    "    with open(processed_data_path, 'rb') as file:\n",
    "        processed_data = pickle.load(file)\n",
    "    for image_id, caption in processed_data:\n",
    "        caption = torch.tensor(caption)\n",
    "        image = image_dict[image_id]\n",
    "        result.append((image, caption))\n",
    "    return result\n",
    "def get_train_test_loader(batch_size: int, n_wokers: int = 2):\n",
    "    image_path = \"../Flickr8k/Flicker8k_Dataset\"\n",
    "    train_path = \"../Data_bert/train_set_bert.pkl\"\n",
    "    test_path = \"../Data_bert/test_set_bert.pkl\"\n",
    "    image_dict: dict[str, torch.Tensor] = load_image(image_path)\n",
    "    train_data = process_data(image_dict, train_path)\n",
    "    test_data = process_data(image_dict, test_path)\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, num_workers=n_wokers, shuffle=True)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, num_workers=n_wokers, shuffle=False)\n",
    "    return trainloader, testloader\n",
    "trainloader, testloader = get_train_test_loader(BATCH_SIZE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, captions in trainloader:\n",
    "#     for i in range(min(4, captions.shape[0])):\n",
    "#         print(images[i][:,100,100])\n",
    "#         print(detokenize(captions[i]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, output_size: int):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.inception_model = models.inception_v3(pretrained=True)\n",
    "        #self.inception_model.fc = torch.nn.Identity()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(1000, output_size)\n",
    "        for name, param in self.inception_model.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    def forward(self, images: torch.Tensor):\n",
    "        features = self.inception_model(images) #[1, 2048]\n",
    "        if isinstance(features, tuple):  # Nếu là tuple\n",
    "            features = features[0] \n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "        features = self.fc(features)\n",
    "        return features\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, hidden_size, input_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_hidden = False\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, hidden_state: tuple[torch.Tensor, torch.Tensor] = None): \n",
    "        # Constant : seq = 1\n",
    "        # features : [bsz, img_sz]\n",
    "        # captions : [bsz, seq]\n",
    "        # hidden : [num_layers, bsz, hidden]\n",
    "        embeddings = self.embed(captions) # [bsz, seq, embed]\n",
    "        features = features.unsqueeze(1).expand(-1, embeddings.shape[1], -1) # [bsz, seq, embed]\n",
    "        combined = torch.cat((features, embeddings), dim=2) # [bsz, seq, img_sz + embed]\n",
    "        # hidden_state : [num_layers, seq, hid] * 2\n",
    "        if self.use_hidden:\n",
    "            output, hidden_state = self.lstm(combined, hidden_state)\n",
    "        else:\n",
    "            output, hidden_state = self.lstm(combined)\n",
    "        # output : [bsz, seq, vocab_size]\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden_state\n",
    "class ImageToTextModel(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
    "        super(ImageToTextModel, self).__init__()\n",
    "        self.encoder: EncoderCNN = encoder\n",
    "        self.decoder: DecoderRNN = decoder\n",
    "    def forward(self, images: torch.Tensor, captions: torch.Tensor):\n",
    "        # Constant : SEQ_LENGTH = 46, seq = 1\n",
    "        # images: [bsz, 3, raw_image_width, raw_image_height]\n",
    "        # captions: [bsz, SEQ_LENGTH]\n",
    "        bsz = images.shape[0]\n",
    "        if self.decoder.use_hidden:\n",
    "            hidden_state: tuple[torch.Tensor, torch.Tensor] = None\n",
    "        features = self.encoder(images)\n",
    "        # features: [bsz, img_sz]\n",
    "        seq_predicted = []\n",
    "        seq_predicted.append(torch.zeros((bsz, self.decoder.vocab_size), dtype=torch.float32).unsqueeze(1).to(device))\n",
    "        # seq_predicted : [predict_length, seq, vocab]\n",
    "        decoder_input = captions[:, 0].unsqueeze(1)\n",
    "        # decoder_input : [bsz, seq]\n",
    "        seq_length = captions.shape[1]\n",
    "        for di in range(1, seq_length):\n",
    "            if self.decoder.use_hidden:\n",
    "                output_decoder, hidden_state = self.decoder(features, decoder_input, hidden_state)\n",
    "            else:\n",
    "                output_decoder, hidden_state = self.decoder(features, decoder_input)\n",
    "            # ouput_decoder: [bsz, seq, vocab]\n",
    "            # hidden_state: [num_layers, bsz, hidden_size] * 2\n",
    "            decoder_input = captions[:, di].unsqueeze(1)\n",
    "            # decoder_input : [bsz, seq]\n",
    "            seq_predicted.append(output_decoder)\n",
    "        return torch.cat(seq_predicted, dim=1)\n",
    "    def predict(self, images: torch.Tensor, captions: torch.Tensor, predict_length: int):\n",
    "        bsz = images.shape[0]\n",
    "        hidden_state: tuple[torch.Tensor, torch.Tensor] = None\n",
    "        features = self.encoder(images)\n",
    "        seq_predicted = []\n",
    "        seq_predicted.append(torch.zeros((bsz, self.decoder.vocab_size), dtype=torch.float32).unsqueeze(1).to(device))\n",
    "        decoder_input = captions\n",
    "        for _ in range(predict_length-1):\n",
    "            output_decoder, hidden_state = self.decoder(features, decoder_input, hidden_state)\n",
    "            seq_predicted.append(output_decoder)\n",
    "            decoder_input = output_decoder.argmax(2)\n",
    "        return torch.cat(seq_predicted, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model : ImageToTextModel, dataloader : DataLoader, lossf : callable, optimizer : torch.optim.Optimizer, mixed: bool, device: str):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    count = 0\n",
    "    for images, captions in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        images: torch.Tensor = images.to(device)\n",
    "        captions: torch.Tensor = captions.to(device)\n",
    "        if mixed:\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                outputs: torch.Tensor = model(images, captions)\n",
    "                loss: torch.Tensor = lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        else:\n",
    "            outputs: torch.Tensor = model(images, captions)\n",
    "            loss: torch.Tensor = lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        count += 1 \n",
    "    return epoch_loss / count\n",
    "def compute_metrics(predict: torch.Tensor, caption: torch.Tensor): # Quên chưa xóa token đầu ...\n",
    "    # Accuracy\n",
    "    is_correct = 1 if predict[1] == caption[1] else 0\n",
    "    caption: list[str] = detokenize(caption)\n",
    "    predict: list[str] = detokenize(predict)\n",
    "    while (caption[-1] == \"[PAD]\"):\n",
    "        caption.pop()\n",
    "    while (predict[-1] == \"[PAD]\"):\n",
    "        predict.pop()\n",
    "    # Bleu\n",
    "    reference = [caption]\n",
    "    # print(caption, predict)\n",
    "    bleu_1 = sentence_bleu(reference, predict, weights=(1, 0, 0, 0))\n",
    "    bleu_2 = sentence_bleu(reference, predict, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu_3 = sentence_bleu(reference, predict, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu_4 = sentence_bleu(reference, predict)\n",
    "    # Rogue\n",
    "    # rogue = Rogue()\n",
    "    # scores = rogue.get_scores(' '.join(predicts), ' '.join(captions))\n",
    "    # Meteor\n",
    "    # meteor_score_ = meteor_score([' '.join(captions)], ' '.join(predicts))\n",
    "    # Cider, spice\n",
    "    # cider_scorer = Cider()\n",
    "    # spice_scorer = Spice()\n",
    "    # cider_score, _ = cider_scorer.compute_score({0 : [' '.join(caption)]}, {0 : [' '.join(predict)]})\n",
    "    # spice_score, _ = spice_scorer.compute_score({0 : [' '.join(captions)]}, {0 : [' '.join(predicts)]})\n",
    "    return {\n",
    "        \"accuracy\" : is_correct,\n",
    "        \"bleu_1\" : bleu_1,\n",
    "        \"bleu_2\" : bleu_2,\n",
    "        \"bleu_3\" : bleu_3,\n",
    "        \"bleu_4\" : bleu_4,\n",
    "        # \"rogue-1\" : scores[\"rogue-1\"][\"f\"],\n",
    "        # \"rogue-2\" : scores[\"rogue-2\"][\"f\"],\n",
    "        # \"rogue-l\" : scores[\"rogue-l\"][\"f\"],\n",
    "        # \"meteor\" : meteor_score_,\n",
    "        # \"cider\" : cider_score,\n",
    "        # \"spice\" : spice_score\n",
    "    }\n",
    "def test(model : ImageToTextModel, dataloader : DataLoader, lossf : callable, mixed: bool, device: str):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    total_count = 0\n",
    "    epoch_metrics = {'loss' : 0}\n",
    "    input = torch.tensor([101]).to(device)\n",
    "    for images, captions in dataloader:\n",
    "        images: torch.Tensor = images.to(device)\n",
    "        captions: torch.Tensor = captions.to(device)\n",
    "        bsz = images.shape[0]\n",
    "        inputs = input.unsqueeze(1).expand((bsz, 1))\n",
    "        if mixed:\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                outputs: torch.Tensor = model.predict(images, inputs, captions.shape[1])\n",
    "                loss: torch.Tensor= lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        else:\n",
    "            outputs: torch.Tensor  = model.predict(images, inputs, captions.shape[1])\n",
    "            loss: torch.Tensor= lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        predicts = outputs.argmax(2)\n",
    "        count += 1\n",
    "        total_count += bsz\n",
    "        epoch_metrics['loss'] += loss.item()\n",
    "        for i in range(bsz):\n",
    "            metrics = compute_metrics(predicts[i], captions[i])\n",
    "            for key in metrics:\n",
    "                if key not in epoch_metrics:\n",
    "                    epoch_metrics[key] = metrics[key]\n",
    "                else:\n",
    "                    epoch_metrics[key] += metrics[key]\n",
    "    for key in epoch_metrics:\n",
    "        if key in ['loss']:\n",
    "            epoch_metrics[key] /= count\n",
    "        else:\n",
    "            epoch_metrics[key] /= total_count\n",
    "    return epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "image_size = 256\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "encoder = EncoderCNN(\n",
    "    output_size=image_size\n",
    ")\n",
    "decoder = DecoderRNN(\n",
    "    embed_size=embed_size,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=hidden_size,\n",
    "    input_size=image_size,\n",
    "    num_layers=1\n",
    ")\n",
    "decoder.use_hidden = True\n",
    "image_to_text_model = ImageToTextModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder\n",
    ")\n",
    "loss_func= nn.CrossEntropyLoss(ignore_index=0)\n",
    "model_path, train_log = util.train_eval(\n",
    "    trainloader=trainloader,\n",
    "    testloader=testloader,\n",
    "    model=image_to_text_model,\n",
    "    train_func=train,\n",
    "    test_func=test,\n",
    "    lossf=loss_func,\n",
    "    num_epochs=10,\n",
    "    lr=2.5e-3,\n",
    "    gamma=0.95,\n",
    "    log_step=1,\n",
    "    warmup_nepochs=10,\n",
    "    warmup_lr=1e-3,\n",
    "    warmup_gamma=1.1,\n",
    "    save=True,\n",
    "    save_optimizer=True,\n",
    "    save_each=5,\n",
    "    mixed_train = True,\n",
    "    mixed_eval = False,\n",
    "    # load_checkpoint = True,\n",
    "    # load_optimizer = True,\n",
    "    # checkpoint_path = \"checkpoint/teset_model/2024-12-12_10-34-10\",\n",
    "    save_path=\"checkpoint/teset_model\",\n",
    "    optimizer_type=torch.optim.Adam,\n",
    "    device=device,\n",
    "    metadata_extra={\n",
    "        \"batch_size\" : BATCH_SIZE,\n",
    "        \"dataset_name\" : \"Flickr8k\",\n",
    "        \"use_hidden\" : decoder.use_hidden\n",
    "    },\n",
    "    log_metric=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = EncoderCNN(\n",
    "#     output_size=image_size\n",
    "# )\n",
    "# decoder = DecoderRNN(\n",
    "#     embed_size=embed_size,\n",
    "#     vocab_size=tokenizer.vocab_size,\n",
    "#     hidden_size=hidden_size,\n",
    "#     input_size=image_size,\n",
    "#     num_layers=1\n",
    "# )\n",
    "# decoder.use_hidden = True\n",
    "# image_to_text_model = ImageToTextModel(\n",
    "#     encoder=encoder,\n",
    "#     decoder=decoder\n",
    "# )\n",
    "# loss_func= nn.CrossEntropyLoss(ignore_index=0)\n",
    "# model_path, train_log = util.train_eval(\n",
    "#     trainloader=trainloader,\n",
    "#     testloader=testloader,\n",
    "#     model=image_to_text_model,\n",
    "#     train_func=train,\n",
    "#     test_func=test,\n",
    "#     lossf=loss_func,\n",
    "#     num_epochs=50,\n",
    "#     lr=2.5e-3,\n",
    "#     gamma=0.95,\n",
    "#     log_step=1,\n",
    "#     warmup_nepochs=10,\n",
    "#     warmup_lr=1e-3,\n",
    "#     warmup_gamma=1.1,\n",
    "#     save=True,\n",
    "#     mixed_train = True,\n",
    "#     mixed_eval = False,\n",
    "#     save_path=\"checkpoint/teset_model\",\n",
    "#     optimizer_type=torch.optim.Adam,\n",
    "#     device=device,\n",
    "#     metadata_extra={\n",
    "#         \"batch_size\" : BATCH_SIZE,\n",
    "#         \"dataset_name\" : \"Flickr8k\",\n",
    "#         \"use_hidden\" : decoder.use_hidden\n",
    "#     },\n",
    "#     log_metric=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "util.save_model_chunks(image_to_text_model, \"checkpoint/un_complete\", \"model\", 95*1024*1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

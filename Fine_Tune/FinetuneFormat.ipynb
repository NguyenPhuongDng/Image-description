{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format lại code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import math\n",
    "import torch.multiprocessing as mp\n",
    "from transformers import BertTokenizerFast\n",
    "import pickle\n",
    "# Evaluate\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# from nltk.translate.meteor_score import meteor_score\n",
    "# from rogue import Rogue\n",
    "# from pycocoevalcap.cider.cider import Cider\n",
    "# from pycocoevalcap.spice.spice import Spice\n",
    "#end\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "detokenize = tokenizer.convert_ids_to_tokens\n",
    "batch_detokenize = tokenizer.batch_decode\n",
    "BATCH_SIZE = 64 \n",
    "#64 : 3.1GB VRAM b16 (3.0 Dedicated | 0.1 Shared)\n",
    "device = 'cuda'\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Dùng cho ImageNet\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loading 10.01 % \n",
      "Image loading 20.02 % \n",
      "Image loading 30.03 % \n",
      "Image loading 40.04 % \n",
      "Image loading 50.06 % \n",
      "Image loading 60.07 % \n",
      "Image loading 70.08 % \n",
      "Image loading 80.09 % \n",
      "Image loading 90.10 % \n",
      "Image loading 100 % \n"
     ]
    }
   ],
   "source": [
    "def load_image(image_folder_path: str): # DataLoader trên Jupyternotebook ko xử lý đa luồng đc nên đành phải load hết vô\n",
    "    result = {}\n",
    "    file_counts = len(os.listdir(image_folder_path))\n",
    "    progress = 0\n",
    "    last_log = 0\n",
    "    count = 0\n",
    "    for file_name in os.listdir(image_folder_path):\n",
    "        file_path = os.path.join(image_folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            id = os.path.splitext(os.path.basename(file_name))[0]\n",
    "            if id in result: continue\n",
    "            with Image.open(file_path).convert(\"RGB\") as img:\n",
    "                image = image_transforms(img)\n",
    "                result[id] = torch.Tensor(image)\n",
    "            count += 1\n",
    "            progress = count / file_counts * 100\n",
    "            if progress - last_log >= 10:\n",
    "                last_log = progress\n",
    "                print(f\"Image loading {progress:.2f} % \")\n",
    "    if last_log != 100:\n",
    "        print(f\"Image loading 100 % \")\n",
    "    return result\n",
    "def process_data(image_dict: dict[str, torch.Tensor], processed_data_path: str):\n",
    "    result: list[tuple[torch.Tensor, torch.Tensor]] = []\n",
    "    with open(processed_data_path, 'rb') as file:\n",
    "        processed_data = pickle.load(file)\n",
    "    for image_id, caption in processed_data:\n",
    "        caption = torch.tensor(caption)\n",
    "        image = image_dict[image_id]\n",
    "        result.append((image, caption))\n",
    "    return result\n",
    "def get_train_test_loader(batch_size: int, n_wokers: int = 2):\n",
    "    image_path = \"../Flickr8k/Flicker8k_Dataset\"\n",
    "    train_path = \"../Data_bert/train_set_bert.pkl\"\n",
    "    test_path = \"../Data_bert/test_set_bert.pkl\"\n",
    "    image_dict: dict[str, torch.Tensor] = load_image(image_path)\n",
    "    train_data = process_data(image_dict, train_path)\n",
    "    test_data = process_data(image_dict, test_path)\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, num_workers=n_wokers, shuffle=True)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, num_workers=n_wokers, shuffle=False)\n",
    "    return trainloader, testloader\n",
    "trainloader, testloader = get_train_test_loader(BATCH_SIZE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, captions in trainloader:\n",
    "#     for i in range(min(4, captions.shape[0])):\n",
    "#         print(images[i][:,100,100])\n",
    "#         print(detokenize(captions[i]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, output_size: int):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.inception_model = models.inception_v3(pretrained=True)\n",
    "        #self.inception_model.fc = torch.nn.Identity()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(1000, output_size)\n",
    "        for name, param in self.inception_model.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    def forward(self, images: torch.Tensor):\n",
    "        features = self.inception_model(images) #[1, 2048]\n",
    "        if isinstance(features, tuple):  # Nếu là tuple\n",
    "            features = features[0] \n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "        features = self.fc(features)\n",
    "        return features\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, hidden_size, input_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_hidden = False\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, hidden_state: tuple[torch.Tensor, torch.Tensor] = None): \n",
    "        # Constant : seq = 1\n",
    "        # features : [bsz, img_sz]\n",
    "        # captions : [bsz, seq]\n",
    "        # hidden : [num_layers, bsz, hidden]\n",
    "        embeddings = self.embed(captions) # [bsz, seq, embed]\n",
    "        features = features.unsqueeze(1).expand(-1, embeddings.shape[1], -1) # [bsz, seq, embed]\n",
    "        combined = torch.cat((features, embeddings), dim=2) # [bsz, seq, img_sz + embed]\n",
    "        # hidden_state : [num_layers, seq, hid] * 2\n",
    "        if self.use_hidden:\n",
    "            output, hidden_state = self.lstm(combined, hidden_state)\n",
    "        else:\n",
    "            output, hidden_state = self.lstm(combined)\n",
    "        # output : [bsz, seq, vocab_size]\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden_state\n",
    "class ImageToTextModel(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
    "        super(ImageToTextModel, self).__init__()\n",
    "        self.encoder: EncoderCNN = encoder\n",
    "        self.decoder: DecoderRNN = decoder\n",
    "    def forward(self, images: torch.Tensor, captions: torch.Tensor):\n",
    "        # Constant : SEQ_LENGTH = 46, seq = 1\n",
    "        # images: [bsz, 3, raw_image_width, raw_image_height]\n",
    "        # captions: [bsz, SEQ_LENGTH]\n",
    "        bsz = images.shape[0]\n",
    "        if self.decoder.use_hidden:\n",
    "            hidden_state: tuple[torch.Tensor, torch.Tensor] = None\n",
    "        features = self.encoder(images)\n",
    "        # features: [bsz, img_sz]\n",
    "        seq_predicted = []\n",
    "        seq_predicted.append(torch.zeros((bsz, self.decoder.vocab_size), dtype=torch.float32).unsqueeze(1).to(device))\n",
    "        # seq_predicted : [predict_length, seq, vocab]\n",
    "        decoder_input = captions[:, 0].unsqueeze(1)\n",
    "        # decoder_input : [bsz, seq]\n",
    "        seq_length = captions.shape[1]\n",
    "        for di in range(1, seq_length):\n",
    "            if self.decoder.use_hidden:\n",
    "                output_decoder, hidden_state = self.decoder(features, decoder_input, hidden_state)\n",
    "            else:\n",
    "                output_decoder, hidden_state = self.decoder(features, decoder_input)\n",
    "            # ouput_decoder: [bsz, seq, vocab]\n",
    "            # hidden_state: [num_layers, bsz, hidden_size] * 2\n",
    "            decoder_input = captions[:, di].unsqueeze(1)\n",
    "            # decoder_input : [bsz, seq]\n",
    "            seq_predicted.append(output_decoder)\n",
    "        return torch.cat(seq_predicted, dim=1)\n",
    "    def predict(self, images: torch.Tensor, captions: torch.Tensor, predict_length: int):\n",
    "        bsz = images.shape[0]\n",
    "        hidden_state: tuple[torch.Tensor, torch.Tensor] = None\n",
    "        features = self.encoder(images)\n",
    "        seq_predicted = []\n",
    "        seq_predicted.append(torch.zeros((bsz, self.decoder.vocab_size), dtype=torch.float32).unsqueeze(1).to(device))\n",
    "        decoder_input = captions\n",
    "        for _ in range(predict_length-1):\n",
    "            output_decoder, hidden_state = self.decoder(features, decoder_input, hidden_state)\n",
    "            seq_predicted.append(output_decoder)\n",
    "            decoder_input = output_decoder.argmax(2)\n",
    "        return torch.cat(seq_predicted, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model : ImageToTextModel, dataloader : DataLoader, lossf : callable, optimizer : torch.optim.Optimizer, mixed: bool, device: str):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    count = 0\n",
    "    for images, captions in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        images: torch.Tensor = images.to(device)\n",
    "        captions: torch.Tensor = captions.to(device)\n",
    "        if mixed:\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                outputs: torch.Tensor = model(images, captions)\n",
    "                loss: torch.Tensor = lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        else:\n",
    "            outputs: torch.Tensor = model(images, captions)\n",
    "            loss: torch.Tensor = lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        count += 1 \n",
    "    return epoch_loss / count\n",
    "def compute_metrics(predict: torch.Tensor, caption: torch.Tensor): # Quên chưa xóa token đầu ...\n",
    "    # Accuracy\n",
    "    is_correct = 1 if predict[1] == caption[1] else 0\n",
    "    caption: list[str] = detokenize(caption)\n",
    "    predict: list[str] = detokenize(predict)\n",
    "    while (caption[-1] == \"[PAD]\"):\n",
    "        caption.pop()\n",
    "    while (predict[-1] == \"[PAD]\"):\n",
    "        predict.pop()\n",
    "    # Bleu\n",
    "    reference = [caption]\n",
    "    # print(caption, predict)\n",
    "    bleu_1 = sentence_bleu(reference, predict, weights=(1, 0, 0, 0))\n",
    "    bleu_2 = sentence_bleu(reference, predict, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu_3 = sentence_bleu(reference, predict, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu_4 = sentence_bleu(reference, predict)\n",
    "    # Rogue\n",
    "    # rogue = Rogue()\n",
    "    # scores = rogue.get_scores(' '.join(predicts), ' '.join(captions))\n",
    "    # Meteor\n",
    "    # meteor_score_ = meteor_score([' '.join(captions)], ' '.join(predicts))\n",
    "    # Cider, spice\n",
    "    # cider_scorer = Cider()\n",
    "    # spice_scorer = Spice()\n",
    "    # cider_score, _ = cider_scorer.compute_score({0 : [' '.join(caption)]}, {0 : [' '.join(predict)]})\n",
    "    # spice_score, _ = spice_scorer.compute_score({0 : [' '.join(captions)]}, {0 : [' '.join(predicts)]})\n",
    "    return {\n",
    "        \"accuracy\" : is_correct,\n",
    "        \"bleu_1\" : bleu_1,\n",
    "        \"bleu_2\" : bleu_2,\n",
    "        \"bleu_3\" : bleu_3,\n",
    "        \"bleu_4\" : bleu_4,\n",
    "        # \"rogue-1\" : scores[\"rogue-1\"][\"f\"],\n",
    "        # \"rogue-2\" : scores[\"rogue-2\"][\"f\"],\n",
    "        # \"rogue-l\" : scores[\"rogue-l\"][\"f\"],\n",
    "        # \"meteor\" : meteor_score_,\n",
    "        # \"cider\" : cider_score,\n",
    "        # \"spice\" : spice_score\n",
    "    }\n",
    "def test(model : ImageToTextModel, dataloader : DataLoader, lossf : callable, mixed: bool, device: str):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    total_count = 0\n",
    "    epoch_metrics = {'loss' : 0}\n",
    "    input = torch.tensor([101]).to(device)\n",
    "    for images, captions in dataloader:\n",
    "        images: torch.Tensor = images.to(device)\n",
    "        captions: torch.Tensor = captions.to(device)\n",
    "        bsz = images.shape[0]\n",
    "        inputs = input.unsqueeze(1).expand((bsz, 1))\n",
    "        if mixed:\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                outputs: torch.Tensor = model.predict(images, inputs, captions.shape[1])\n",
    "                loss: torch.Tensor= lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        else:\n",
    "            outputs: torch.Tensor  = model.predict(images, inputs, captions.shape[1])\n",
    "            loss: torch.Tensor= lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        predicts = outputs.argmax(2)\n",
    "        count += 1\n",
    "        total_count += bsz\n",
    "        epoch_metrics['loss'] += loss.item()\n",
    "        for i in range(bsz):\n",
    "            metrics = compute_metrics(predicts[i], captions[i])\n",
    "            for key in metrics:\n",
    "                if key not in epoch_metrics:\n",
    "                    epoch_metrics[key] = metrics[key]\n",
    "                else:\n",
    "                    epoch_metrics[key] += metrics[key]\n",
    "    for key in epoch_metrics:\n",
    "        if key in ['loss']:\n",
    "            epoch_metrics[key] /= count\n",
    "        else:\n",
    "            epoch_metrics[key] /= total_count\n",
    "    return epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint\n",
      "Load optimizer\n",
      "Start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]c:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 2.8423 | Test loss : 9.7687 | Train time : 225.34 s | Lr : 0.00150000\n",
      "{'loss': 9.768658613856834, 'accuracy': 0.615, 'bleu_1': 0.10928695652174165, 'bleu_2': 0.06175941310102131, 'bleu_3': 0.013768048539602762, 'bleu_4': 0.005707871733790148}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [04:40<42:03, 280.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 2.8207 | Test loss : 9.8464 | Train time : 207.01 s | Lr : 0.00142500\n",
      "{'loss': 9.846429571320739, 'accuracy': 0.615, 'bleu_1': 0.10533913043478549, 'bleu_2': 0.0589856288521118, 'bleu_3': 0.013322871959321555, 'bleu_4': 0.005078283751941555}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [08:52<35:11, 263.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 2.7979 | Test loss : 9.9149 | Train time : 214.85 s | Lr : 0.00135375\n",
      "{'loss': 9.91486482982394, 'accuracy': 0.615, 'bleu_1': 0.10990869565217631, 'bleu_2': 0.06306358767601701, 'bleu_3': 0.014367511375610502, 'bleu_4': 0.006096348507916821}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [13:13<30:38, 262.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 2.7770 | Test loss : 9.9156 | Train time : 195.46 s | Lr : 0.00128606\n",
      "{'loss': 9.915596358383759, 'accuracy': 0.615, 'bleu_1': 0.10929130434782858, 'bleu_2': 0.062248435751013735, 'bleu_3': 0.014177038337637487, 'bleu_4': 0.005513077935003668}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [17:17<25:29, 254.96s/it]"
     ]
    }
   ],
   "source": [
    "import util\n",
    "image_size = 256\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "encoder = EncoderCNN(\n",
    "    output_size=image_size\n",
    ")\n",
    "decoder = DecoderRNN(\n",
    "    embed_size=embed_size,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=hidden_size,\n",
    "    input_size=image_size,\n",
    "    num_layers=1\n",
    ")\n",
    "decoder.use_hidden = True\n",
    "image_to_text_model = ImageToTextModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder\n",
    ")\n",
    "loss_func= nn.CrossEntropyLoss(ignore_index=0)\n",
    "model_path, train_log = util.train_eval(\n",
    "    trainloader=trainloader,\n",
    "    testloader=testloader,\n",
    "    model=image_to_text_model,\n",
    "    train_func=train,\n",
    "    test_func=test,\n",
    "    lossf=loss_func,\n",
    "    num_epochs=10,\n",
    "    lr=1.5e-3,\n",
    "    gamma=0.95,\n",
    "    log_step=1,\n",
    "    warmup_nepochs=0,\n",
    "    warmup_lr=1e-3,\n",
    "    warmup_gamma=1.1,\n",
    "    save=True,\n",
    "    save_optimizer=True,\n",
    "    save_each=1,\n",
    "    mixed_train = True,\n",
    "    mixed_eval = False,\n",
    "    load_checkpoint = True,\n",
    "    load_optimizer = True,\n",
    "    checkpoint_path = \"checkpoint/teset_model/2024-12-12_14-28-55\",\n",
    "    save_path=\"checkpoint/teset_model\",\n",
    "    optimizer_type=torch.optim.Adam,\n",
    "    device=device,\n",
    "    metadata_extra={\n",
    "        \"batch_size\" : BATCH_SIZE,\n",
    "        \"dataset_name\" : \"Flickr8k\",\n",
    "        \"use_hidden\" : decoder.use_hidden\n",
    "    },\n",
    "    log_metric=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = EncoderCNN(\n",
    "#     output_size=image_size\n",
    "# )\n",
    "# decoder = DecoderRNN(\n",
    "#     embed_size=embed_size,\n",
    "#     vocab_size=tokenizer.vocab_size,\n",
    "#     hidden_size=hidden_size,\n",
    "#     input_size=image_size,\n",
    "#     num_layers=1\n",
    "# )\n",
    "# decoder.use_hidden = True\n",
    "# image_to_text_model = ImageToTextModel(\n",
    "#     encoder=encoder,\n",
    "#     decoder=decoder\n",
    "# )\n",
    "# loss_func= nn.CrossEntropyLoss(ignore_index=0)\n",
    "# model_path, train_log = util.train_eval(\n",
    "#     trainloader=trainloader,\n",
    "#     testloader=testloader,\n",
    "#     model=image_to_text_model,\n",
    "#     train_func=train,\n",
    "#     test_func=test,\n",
    "#     lossf=loss_func,\n",
    "#     num_epochs=50,\n",
    "#     lr=2.5e-3,\n",
    "#     gamma=0.95,\n",
    "#     log_step=1,\n",
    "#     warmup_nepochs=10,\n",
    "#     warmup_lr=1e-3,\n",
    "#     warmup_gamma=1.1,\n",
    "#     save=True,\n",
    "#     mixed_train = True,\n",
    "#     mixed_eval = False,\n",
    "#     save_path=\"checkpoint/teset_model\",\n",
    "#     optimizer_type=torch.optim.Adam,\n",
    "#     device=device,\n",
    "#     metadata_extra={\n",
    "#         \"batch_size\" : BATCH_SIZE,\n",
    "#         \"dataset_name\" : \"Flickr8k\",\n",
    "#         \"use_hidden\" : decoder.use_hidden\n",
    "#     },\n",
    "#     log_metric=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "util.save_model_chunks(image_to_text_model, \"checkpoint/un_complete\", \"model\", 95*1024*1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format lại code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from pickle import dump, load\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import math\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "BATCH_SIZE = 64\n",
    "device = 'cuda'\n",
    "SEQ_LENGTH = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((324, 324)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Dùng cho ImageNet\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Flickr8kDataset.__init__() got an unexpected keyword argument 'seq_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m     testloader \u001b[38;5;241m=\u001b[39m DataLoader(final_test_data, batch_size \u001b[38;5;241m=\u001b[39m BATCH_SIZE, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainloader, testloader\n\u001b[1;32m---> 44\u001b[0m trainloader, testloader \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m\n\u001b[0;32m      3\u001b[0m data_folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlickr8k/Flicker8k_Dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFlickr8kDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_folder_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData_bert/train_set_bert.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_transforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEQ_LENGTH\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mget_dataloader(batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mFlickr8kDataset(\n\u001b[0;32m     13\u001b[0m     data_folder_path,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData_bert/test_set_bert.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     seq_length\u001b[38;5;241m=\u001b[39mSEQ_LENGTH\n\u001b[0;32m     18\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: Flickr8kDataset.__init__() got an unexpected keyword argument 'seq_length'"
     ]
    }
   ],
   "source": [
    "def get_data(): # Do dataloader bị lỗi đa luồng ...\n",
    "    import util\n",
    "    data_folder_path = \"Flickr8k/Flicker8k_Dataset\"\n",
    "    train_dataset = util.Flickr8kDataset(\n",
    "        data_folder_path,\n",
    "        \"Data_bert/train_set_bert.pkl\",\n",
    "        image_transforms,\n",
    "        device='cpu',\n",
    "        seq_length=SEQ_LENGTH\n",
    "    )\n",
    "    trainloader = train_dataset.get_dataloader(batch_size=BATCH_SIZE, num_workers=0, shuffle=False)\n",
    "    test_dataset = util.Flickr8kDataset(\n",
    "        data_folder_path,\n",
    "        \"Data_bert/test_set_bert.pkl\",\n",
    "        image_transforms,\n",
    "        device='cpu',\n",
    "        seq_length=SEQ_LENGTH\n",
    "    )\n",
    "    testloader = test_dataset.get_dataloader(batch_size=BATCH_SIZE, num_workers=0, shuffle=False)\n",
    "    print(len(train_dataset))\n",
    "    print(len(test_dataset)) \n",
    "    sample_image, sample_caption, sample_target = test_dataset[0]\n",
    "    print(sample_image.shape, sample_caption.shape, sample_target.shape)\n",
    "    print(sample_caption)\n",
    "    print(sample_target)\n",
    "    final_train_data = []\n",
    "    progress = 0\n",
    "    last_log = 0\n",
    "    for images, captions, targets in trainloader:\n",
    "        for i in range(len(images)):\n",
    "            final_train_data.append((images[i], captions[i], targets[i]))\n",
    "        progress += 1\n",
    "        current = progress / len(trainloader) * 100\n",
    "        if ( current - last_log > 5):\n",
    "            last_log = current\n",
    "            print(f\"Train {current} %\")\n",
    "    final_test_data = []\n",
    "    for images, captions, targets in testloader:\n",
    "        for i in range(len(images)):\n",
    "            final_test_data.append((images[i], captions[i], targets[i]))\n",
    "    trainloader = DataLoader(final_train_data, batch_size = BATCH_SIZE, num_workers=2, shuffle=False)\n",
    "    testloader = DataLoader(final_test_data, batch_size = BATCH_SIZE, num_workers=2, shuffle=False)\n",
    "    return trainloader, testloader\n",
    "trainloader, testloader = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'a', 'child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an']\n",
      "- ['a', 'child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry']\n",
      "['a', 'child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry']\n",
      "- ['child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way']\n",
      "['child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way']\n",
      "- ['in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way', '.']\n",
      "['in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way', '.']\n",
      "- ['a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'black', 'dog', 'and', 'a', 'tri', '-', 'colored', 'dog', 'playing', 'with', 'each', 'other', 'on', 'the']\n",
      "- ['a', 'black', 'dog', 'and', 'a', 'tri', '-', 'colored', 'dog', 'playing', 'with', 'each', 'other', 'on', 'the', 'road']\n",
      "['a', 'black', 'dog', 'and', 'a', 'tri', '-', 'colored', 'dog', 'playing', 'with', 'each', 'other', 'on', 'the', 'road']\n",
      "- ['black', 'dog', 'and', 'a', 'tri', '-', 'colored', 'dog', 'playing', 'with', 'each', 'other', 'on', 'the', 'road', '.']\n",
      "['black', 'dog', 'and', 'a', 'tri', '-', 'colored', 'dog', 'playing', 'with', 'each', 'other', 'on', 'the', 'road', '.']\n",
      "- ['dog', 'and', 'a', 'tri', '-', 'colored', 'dog', 'playing', 'with', 'each', 'other', 'on', 'the', 'road', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'black', 'dog', 'and', 'a', 'white', 'dog', 'with', 'brown', 'spots', 'are', 'staring', 'at', 'each', 'other']\n",
      "- ['a', 'black', 'dog', 'and', 'a', 'white', 'dog', 'with', 'brown', 'spots', 'are', 'staring', 'at', 'each', 'other', 'in']\n",
      "['a', 'black', 'dog', 'and', 'a', 'white', 'dog', 'with', 'brown', 'spots', 'are', 'staring', 'at', 'each', 'other', 'in']\n",
      "- ['black', 'dog', 'and', 'a', 'white', 'dog', 'with', 'brown', 'spots', 'are', 'staring', 'at', 'each', 'other', 'in', 'the']\n",
      "['black', 'dog', 'and', 'a', 'white', 'dog', 'with', 'brown', 'spots', 'are', 'staring', 'at', 'each', 'other', 'in', 'the']\n",
      "- ['dog', 'and', 'a', 'white', 'dog', 'with', 'brown', 'spots', 'are', 'staring', 'at', 'each', 'other', 'in', 'the', 'street']\n",
      "['dog', 'and', 'a', 'white', 'dog', 'with', 'brown', 'spots', 'are', 'staring', 'at', 'each', 'other', 'in', 'the', 'street']\n",
      "- ['and', 'a', 'white', 'dog', 'with', 'brown', 'spots', 'are', 'staring', 'at', 'each', 'other', 'in', 'the', 'street', '.']\n",
      "['and', 'a', 'white', 'dog', 'with', 'brown', 'spots', 'are', 'staring', 'at', 'each', 'other', 'in', 'the', 'street', '.']\n",
      "- ['a', 'white', 'dog', 'with', 'brown', 'spots', 'are', 'staring', 'at', 'each', 'other', 'in', 'the', 'street', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'little', 'girl', 'covered', 'in', 'paint', 'sits', 'in', 'front', 'of', 'a', 'painted', 'rainbow', 'with', 'her']\n",
      "- ['a', 'little', 'girl', 'covered', 'in', 'paint', 'sits', 'in', 'front', 'of', 'a', 'painted', 'rainbow', 'with', 'her', 'hands']\n",
      "['a', 'little', 'girl', 'covered', 'in', 'paint', 'sits', 'in', 'front', 'of', 'a', 'painted', 'rainbow', 'with', 'her', 'hands']\n",
      "- ['little', 'girl', 'covered', 'in', 'paint', 'sits', 'in', 'front', 'of', 'a', 'painted', 'rainbow', 'with', 'her', 'hands', 'in']\n",
      "['little', 'girl', 'covered', 'in', 'paint', 'sits', 'in', 'front', 'of', 'a', 'painted', 'rainbow', 'with', 'her', 'hands', 'in']\n",
      "- ['girl', 'covered', 'in', 'paint', 'sits', 'in', 'front', 'of', 'a', 'painted', 'rainbow', 'with', 'her', 'hands', 'in', 'a']\n",
      "['girl', 'covered', 'in', 'paint', 'sits', 'in', 'front', 'of', 'a', 'painted', 'rainbow', 'with', 'her', 'hands', 'in', 'a']\n",
      "- ['covered', 'in', 'paint', 'sits', 'in', 'front', 'of', 'a', 'painted', 'rainbow', 'with', 'her', 'hands', 'in', 'a', 'bowl']\n",
      "['covered', 'in', 'paint', 'sits', 'in', 'front', 'of', 'a', 'painted', 'rainbow', 'with', 'her', 'hands', 'in', 'a', 'bowl']\n",
      "- ['in', 'paint', 'sits', 'in', 'front', 'of', 'a', 'painted', 'rainbow', 'with', 'her', 'hands', 'in', 'a', 'bowl', '.']\n",
      "['in', 'paint', 'sits', 'in', 'front', 'of', 'a', 'painted', 'rainbow', 'with', 'her', 'hands', 'in', 'a', 'bowl', '.']\n",
      "- ['paint', 'sits', 'in', 'front', 'of', 'a', 'painted', 'rainbow', 'with', 'her', 'hands', 'in', 'a', 'bowl', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'small', 'girl', 'in', 'the', 'grass', 'plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of']\n",
      "- ['a', 'small', 'girl', 'in', 'the', 'grass', 'plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a']\n",
      "['a', 'small', 'girl', 'in', 'the', 'grass', 'plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a']\n",
      "- ['small', 'girl', 'in', 'the', 'grass', 'plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white']\n",
      "['small', 'girl', 'in', 'the', 'grass', 'plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white']\n",
      "- ['girl', 'in', 'the', 'grass', 'plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas']\n",
      "['girl', 'in', 'the', 'grass', 'plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas']\n",
      "- ['in', 'the', 'grass', 'plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas', 'with']\n",
      "['in', 'the', 'grass', 'plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas', 'with']\n",
      "- ['the', 'grass', 'plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas', 'with', 'a']\n",
      "['the', 'grass', 'plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas', 'with', 'a']\n",
      "- ['grass', 'plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas', 'with', 'a', 'rainbow']\n",
      "['grass', 'plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas', 'with', 'a', 'rainbow']\n",
      "- ['plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas', 'with', 'a', 'rainbow', 'on']\n",
      "['plays', 'with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas', 'with', 'a', 'rainbow', 'on']\n",
      "- ['with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas', 'with', 'a', 'rainbow', 'on', 'it']\n",
      "['with', 'finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas', 'with', 'a', 'rainbow', 'on', 'it']\n",
      "- ['finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas', 'with', 'a', 'rainbow', 'on', 'it', '.']\n",
      "['finger', '##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas', 'with', 'a', 'rainbow', 'on', 'it', '.']\n",
      "- ['##pa', '##int', '##s', 'in', 'front', 'of', 'a', 'white', 'canvas', 'with', 'a', 'rainbow', 'on', 'it', '.', '[SEP]']\n",
      "['[CLS]', 'there', 'is', 'a', 'girl', 'with', 'pig', '##tails', 'sitting', 'in', 'front', 'of', 'a', 'rainbow', 'painting', '.']\n",
      "- ['there', 'is', 'a', 'girl', 'with', 'pig', '##tails', 'sitting', 'in', 'front', 'of', 'a', 'rainbow', 'painting', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'man', 'lays', 'on', 'the', 'bench', 'to', 'which', 'a', 'white', 'dog', 'is', 'also', 'tied', '.']\n",
      "- ['a', 'man', 'lays', 'on', 'the', 'bench', 'to', 'which', 'a', 'white', 'dog', 'is', 'also', 'tied', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'man', 'sleeping', 'on', 'a', 'bench', 'outside', 'with', 'a', 'white', 'and', 'black', 'dog', 'sitting', 'next']\n",
      "- ['a', 'man', 'sleeping', 'on', 'a', 'bench', 'outside', 'with', 'a', 'white', 'and', 'black', 'dog', 'sitting', 'next', 'to']\n",
      "['a', 'man', 'sleeping', 'on', 'a', 'bench', 'outside', 'with', 'a', 'white', 'and', 'black', 'dog', 'sitting', 'next', 'to']\n",
      "- ['man', 'sleeping', 'on', 'a', 'bench', 'outside', 'with', 'a', 'white', 'and', 'black', 'dog', 'sitting', 'next', 'to', 'him']\n",
      "['man', 'sleeping', 'on', 'a', 'bench', 'outside', 'with', 'a', 'white', 'and', 'black', 'dog', 'sitting', 'next', 'to', 'him']\n",
      "- ['sleeping', 'on', 'a', 'bench', 'outside', 'with', 'a', 'white', 'and', 'black', 'dog', 'sitting', 'next', 'to', 'him', '.']\n",
      "['sleeping', 'on', 'a', 'bench', 'outside', 'with', 'a', 'white', 'and', 'black', 'dog', 'sitting', 'next', 'to', 'him', '.']\n",
      "- ['on', 'a', 'bench', 'outside', 'with', 'a', 'white', 'and', 'black', 'dog', 'sitting', 'next', 'to', 'him', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'black', 'and', 'white', 'dog', 'is', 'running', 'in', 'a', 'grassy', 'garden', 'surrounded', 'by', 'a', 'white']\n",
      "- ['a', 'black', 'and', 'white', 'dog', 'is', 'running', 'in', 'a', 'grassy', 'garden', 'surrounded', 'by', 'a', 'white', 'fence']\n",
      "['a', 'black', 'and', 'white', 'dog', 'is', 'running', 'in', 'a', 'grassy', 'garden', 'surrounded', 'by', 'a', 'white', 'fence']\n",
      "- ['black', 'and', 'white', 'dog', 'is', 'running', 'in', 'a', 'grassy', 'garden', 'surrounded', 'by', 'a', 'white', 'fence', '.']\n",
      "['black', 'and', 'white', 'dog', 'is', 'running', 'in', 'a', 'grassy', 'garden', 'surrounded', 'by', 'a', 'white', 'fence', '.']\n",
      "- ['and', 'white', 'dog', 'is', 'running', 'in', 'a', 'grassy', 'garden', 'surrounded', 'by', 'a', 'white', 'fence', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'boston', 'terri', '##er', 'is', 'running', 'on', 'lush', 'green', 'grass', 'in', 'front', 'of', 'a', 'white']\n",
      "- ['a', 'boston', 'terri', '##er', 'is', 'running', 'on', 'lush', 'green', 'grass', 'in', 'front', 'of', 'a', 'white', 'fence']\n",
      "['a', 'boston', 'terri', '##er', 'is', 'running', 'on', 'lush', 'green', 'grass', 'in', 'front', 'of', 'a', 'white', 'fence']\n",
      "- ['boston', 'terri', '##er', 'is', 'running', 'on', 'lush', 'green', 'grass', 'in', 'front', 'of', 'a', 'white', 'fence', '.']\n",
      "['boston', 'terri', '##er', 'is', 'running', 'on', 'lush', 'green', 'grass', 'in', 'front', 'of', 'a', 'white', 'fence', '.']\n",
      "- ['terri', '##er', 'is', 'running', 'on', 'lush', 'green', 'grass', 'in', 'front', 'of', 'a', 'white', 'fence', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'dog', 'shakes', 'its', 'head', 'near', 'the', 'shore', ',', 'a', 'red', 'ball', 'next', 'to', 'it']\n",
      "- ['a', 'dog', 'shakes', 'its', 'head', 'near', 'the', 'shore', ',', 'a', 'red', 'ball', 'next', 'to', 'it', '.']\n",
      "['a', 'dog', 'shakes', 'its', 'head', 'near', 'the', 'shore', ',', 'a', 'red', 'ball', 'next', 'to', 'it', '.']\n",
      "- ['dog', 'shakes', 'its', 'head', 'near', 'the', 'shore', ',', 'a', 'red', 'ball', 'next', 'to', 'it', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'white', 'dog', 'shakes', 'on', 'the', 'edge', 'of', 'a', 'beach', 'with', 'an', 'orange', 'ball', '.']\n",
      "- ['a', 'white', 'dog', 'shakes', 'on', 'the', 'edge', 'of', 'a', 'beach', 'with', 'an', 'orange', 'ball', '.', '[SEP]']\n",
      "['[CLS]', 'white', 'dog', 'with', 'brown', 'ears', 'standing', 'near', 'water', 'with', 'head', 'turned', 'to', 'one', 'side', '.']\n",
      "- ['white', 'dog', 'with', 'brown', 'ears', 'standing', 'near', 'water', 'with', 'head', 'turned', 'to', 'one', 'side', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'little', 'boy', 'is', 'standing', 'on', 'the', 'street', 'while', 'a', 'man', 'in', 'overall', '##s', 'is']\n",
      "- ['a', 'little', 'boy', 'is', 'standing', 'on', 'the', 'street', 'while', 'a', 'man', 'in', 'overall', '##s', 'is', 'working']\n",
      "['a', 'little', 'boy', 'is', 'standing', 'on', 'the', 'street', 'while', 'a', 'man', 'in', 'overall', '##s', 'is', 'working']\n",
      "- ['little', 'boy', 'is', 'standing', 'on', 'the', 'street', 'while', 'a', 'man', 'in', 'overall', '##s', 'is', 'working', 'on']\n",
      "['little', 'boy', 'is', 'standing', 'on', 'the', 'street', 'while', 'a', 'man', 'in', 'overall', '##s', 'is', 'working', 'on']\n",
      "- ['boy', 'is', 'standing', 'on', 'the', 'street', 'while', 'a', 'man', 'in', 'overall', '##s', 'is', 'working', 'on', 'a']\n",
      "['boy', 'is', 'standing', 'on', 'the', 'street', 'while', 'a', 'man', 'in', 'overall', '##s', 'is', 'working', 'on', 'a']\n",
      "- ['is', 'standing', 'on', 'the', 'street', 'while', 'a', 'man', 'in', 'overall', '##s', 'is', 'working', 'on', 'a', 'stone']\n",
      "['is', 'standing', 'on', 'the', 'street', 'while', 'a', 'man', 'in', 'overall', '##s', 'is', 'working', 'on', 'a', 'stone']\n",
      "- ['standing', 'on', 'the', 'street', 'while', 'a', 'man', 'in', 'overall', '##s', 'is', 'working', 'on', 'a', 'stone', 'wall']\n",
      "['standing', 'on', 'the', 'street', 'while', 'a', 'man', 'in', 'overall', '##s', 'is', 'working', 'on', 'a', 'stone', 'wall']\n",
      "- ['on', 'the', 'street', 'while', 'a', 'man', 'in', 'overall', '##s', 'is', 'working', 'on', 'a', 'stone', 'wall', '.']\n",
      "['on', 'the', 'street', 'while', 'a', 'man', 'in', 'overall', '##s', 'is', 'working', 'on', 'a', 'stone', 'wall', '.']\n",
      "- ['the', 'street', 'while', 'a', 'man', 'in', 'overall', '##s', 'is', 'working', 'on', 'a', 'stone', 'wall', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'young', 'child', 'is', 'walking', 'on', 'a', 'stone', 'paved', 'street', 'with', 'a', 'metal', 'pole', 'and']\n",
      "- ['a', 'young', 'child', 'is', 'walking', 'on', 'a', 'stone', 'paved', 'street', 'with', 'a', 'metal', 'pole', 'and', 'a']\n",
      "['a', 'young', 'child', 'is', 'walking', 'on', 'a', 'stone', 'paved', 'street', 'with', 'a', 'metal', 'pole', 'and', 'a']\n",
      "- ['young', 'child', 'is', 'walking', 'on', 'a', 'stone', 'paved', 'street', 'with', 'a', 'metal', 'pole', 'and', 'a', 'man']\n",
      "['young', 'child', 'is', 'walking', 'on', 'a', 'stone', 'paved', 'street', 'with', 'a', 'metal', 'pole', 'and', 'a', 'man']\n",
      "- ['child', 'is', 'walking', 'on', 'a', 'stone', 'paved', 'street', 'with', 'a', 'metal', 'pole', 'and', 'a', 'man', 'behind']\n",
      "['child', 'is', 'walking', 'on', 'a', 'stone', 'paved', 'street', 'with', 'a', 'metal', 'pole', 'and', 'a', 'man', 'behind']\n",
      "- ['is', 'walking', 'on', 'a', 'stone', 'paved', 'street', 'with', 'a', 'metal', 'pole', 'and', 'a', 'man', 'behind', 'him']\n",
      "['is', 'walking', 'on', 'a', 'stone', 'paved', 'street', 'with', 'a', 'metal', 'pole', 'and', 'a', 'man', 'behind', 'him']\n",
      "- ['walking', 'on', 'a', 'stone', 'paved', 'street', 'with', 'a', 'metal', 'pole', 'and', 'a', 'man', 'behind', 'him', '.']\n",
      "['walking', 'on', 'a', 'stone', 'paved', 'street', 'with', 'a', 'metal', 'pole', 'and', 'a', 'man', 'behind', 'him', '.']\n",
      "- ['on', 'a', 'stone', 'paved', 'street', 'with', 'a', 'metal', 'pole', 'and', 'a', 'man', 'behind', 'him', '.', '[SEP]']\n",
      "['[CLS]', 'smiling', 'boy', 'in', 'white', 'shirt', 'and', 'blue', 'jeans', 'in', 'front', 'of', 'rock', 'wall', 'with', 'man']\n",
      "- ['smiling', 'boy', 'in', 'white', 'shirt', 'and', 'blue', 'jeans', 'in', 'front', 'of', 'rock', 'wall', 'with', 'man', 'in']\n",
      "['smiling', 'boy', 'in', 'white', 'shirt', 'and', 'blue', 'jeans', 'in', 'front', 'of', 'rock', 'wall', 'with', 'man', 'in']\n",
      "- ['boy', 'in', 'white', 'shirt', 'and', 'blue', 'jeans', 'in', 'front', 'of', 'rock', 'wall', 'with', 'man', 'in', 'overall']\n",
      "['boy', 'in', 'white', 'shirt', 'and', 'blue', 'jeans', 'in', 'front', 'of', 'rock', 'wall', 'with', 'man', 'in', 'overall']\n",
      "- ['in', 'white', 'shirt', 'and', 'blue', 'jeans', 'in', 'front', 'of', 'rock', 'wall', 'with', 'man', 'in', 'overall', '##s']\n",
      "['in', 'white', 'shirt', 'and', 'blue', 'jeans', 'in', 'front', 'of', 'rock', 'wall', 'with', 'man', 'in', 'overall', '##s']\n",
      "- ['white', 'shirt', 'and', 'blue', 'jeans', 'in', 'front', 'of', 'rock', 'wall', 'with', 'man', 'in', 'overall', '##s', 'behind']\n",
      "['white', 'shirt', 'and', 'blue', 'jeans', 'in', 'front', 'of', 'rock', 'wall', 'with', 'man', 'in', 'overall', '##s', 'behind']\n",
      "- ['shirt', 'and', 'blue', 'jeans', 'in', 'front', 'of', 'rock', 'wall', 'with', 'man', 'in', 'overall', '##s', 'behind', 'him']\n",
      "['shirt', 'and', 'blue', 'jeans', 'in', 'front', 'of', 'rock', 'wall', 'with', 'man', 'in', 'overall', '##s', 'behind', 'him']\n",
      "- ['and', 'blue', 'jeans', 'in', 'front', 'of', 'rock', 'wall', 'with', 'man', 'in', 'overall', '##s', 'behind', 'him', '.']\n",
      "['and', 'blue', 'jeans', 'in', 'front', 'of', 'rock', 'wall', 'with', 'man', 'in', 'overall', '##s', 'behind', 'him', '.']\n",
      "- ['blue', 'jeans', 'in', 'front', 'of', 'rock', 'wall', 'with', 'man', 'in', 'overall', '##s', 'behind', 'him', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'mo', '##ttle', '##d', 'black', 'and', 'grey', 'dog', 'in', 'a', 'blue', 'collar', 'jumping', 'over', 'a']\n",
      "- ['a', 'mo', '##ttle', '##d', 'black', 'and', 'grey', 'dog', 'in', 'a', 'blue', 'collar', 'jumping', 'over', 'a', 'fallen']\n",
      "['a', 'mo', '##ttle', '##d', 'black', 'and', 'grey', 'dog', 'in', 'a', 'blue', 'collar', 'jumping', 'over', 'a', 'fallen']\n",
      "- ['mo', '##ttle', '##d', 'black', 'and', 'grey', 'dog', 'in', 'a', 'blue', 'collar', 'jumping', 'over', 'a', 'fallen', 'tree']\n",
      "['mo', '##ttle', '##d', 'black', 'and', 'grey', 'dog', 'in', 'a', 'blue', 'collar', 'jumping', 'over', 'a', 'fallen', 'tree']\n",
      "- ['##ttle', '##d', 'black', 'and', 'grey', 'dog', 'in', 'a', 'blue', 'collar', 'jumping', 'over', 'a', 'fallen', 'tree', '.']\n",
      "['##ttle', '##d', 'black', 'and', 'grey', 'dog', 'in', 'a', 'blue', 'collar', 'jumping', 'over', 'a', 'fallen', 'tree', '.']\n",
      "- ['##d', 'black', 'and', 'grey', 'dog', 'in', 'a', 'blue', 'collar', 'jumping', 'over', 'a', 'fallen', 'tree', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'man', 'in', 'a', 'hat', 'is', 'displaying', 'pictures', 'next', 'to', 'a', 'skier', 'in', 'a', 'blue']\n",
      "- ['a', 'man', 'in', 'a', 'hat', 'is', 'displaying', 'pictures', 'next', 'to', 'a', 'skier', 'in', 'a', 'blue', 'hat']\n",
      "['a', 'man', 'in', 'a', 'hat', 'is', 'displaying', 'pictures', 'next', 'to', 'a', 'skier', 'in', 'a', 'blue', 'hat']\n",
      "- ['man', 'in', 'a', 'hat', 'is', 'displaying', 'pictures', 'next', 'to', 'a', 'skier', 'in', 'a', 'blue', 'hat', '.']\n",
      "['man', 'in', 'a', 'hat', 'is', 'displaying', 'pictures', 'next', 'to', 'a', 'skier', 'in', 'a', 'blue', 'hat', '.']\n",
      "- ['in', 'a', 'hat', 'is', 'displaying', 'pictures', 'next', 'to', 'a', 'skier', 'in', 'a', 'blue', 'hat', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'person', 'wearing', 'ski', '##s', 'looking', 'at', 'framed', 'pictures', 'set', 'up', 'in', 'the', 'snow', '.']\n",
      "- ['a', 'person', 'wearing', 'ski', '##s', 'looking', 'at', 'framed', 'pictures', 'set', 'up', 'in', 'the', 'snow', '.', '[SEP]']\n",
      "['[CLS]', 'seven', 'climb', '##ers', 'are', 'ascending', 'a', 'rock', 'face', 'whilst', 'another', 'man', 'stands', 'holding', 'the', 'rope']\n",
      "- ['seven', 'climb', '##ers', 'are', 'ascending', 'a', 'rock', 'face', 'whilst', 'another', 'man', 'stands', 'holding', 'the', 'rope', '.']\n",
      "['seven', 'climb', '##ers', 'are', 'ascending', 'a', 'rock', 'face', 'whilst', 'another', 'man', 'stands', 'holding', 'the', 'rope', '.']\n",
      "- ['climb', '##ers', 'are', 'ascending', 'a', 'rock', 'face', 'whilst', 'another', 'man', 'stands', 'holding', 'the', 'rope', '.', '[SEP]']\n",
      "['[CLS]', 'several', 'climb', '##ers', 'in', 'a', 'row', 'are', 'climbing', 'the', 'rock', 'while', 'the', 'man', 'in', 'red']\n",
      "- ['several', 'climb', '##ers', 'in', 'a', 'row', 'are', 'climbing', 'the', 'rock', 'while', 'the', 'man', 'in', 'red', 'watches']\n",
      "['several', 'climb', '##ers', 'in', 'a', 'row', 'are', 'climbing', 'the', 'rock', 'while', 'the', 'man', 'in', 'red', 'watches']\n",
      "- ['climb', '##ers', 'in', 'a', 'row', 'are', 'climbing', 'the', 'rock', 'while', 'the', 'man', 'in', 'red', 'watches', 'and']\n",
      "['climb', '##ers', 'in', 'a', 'row', 'are', 'climbing', 'the', 'rock', 'while', 'the', 'man', 'in', 'red', 'watches', 'and']\n",
      "- ['##ers', 'in', 'a', 'row', 'are', 'climbing', 'the', 'rock', 'while', 'the', 'man', 'in', 'red', 'watches', 'and', 'holds']\n",
      "['##ers', 'in', 'a', 'row', 'are', 'climbing', 'the', 'rock', 'while', 'the', 'man', 'in', 'red', 'watches', 'and', 'holds']\n",
      "- ['in', 'a', 'row', 'are', 'climbing', 'the', 'rock', 'while', 'the', 'man', 'in', 'red', 'watches', 'and', 'holds', 'the']\n",
      "['in', 'a', 'row', 'are', 'climbing', 'the', 'rock', 'while', 'the', 'man', 'in', 'red', 'watches', 'and', 'holds', 'the']\n",
      "- ['a', 'row', 'are', 'climbing', 'the', 'rock', 'while', 'the', 'man', 'in', 'red', 'watches', 'and', 'holds', 'the', 'line']\n",
      "['a', 'row', 'are', 'climbing', 'the', 'rock', 'while', 'the', 'man', 'in', 'red', 'watches', 'and', 'holds', 'the', 'line']\n",
      "- ['row', 'are', 'climbing', 'the', 'rock', 'while', 'the', 'man', 'in', 'red', 'watches', 'and', 'holds', 'the', 'line', '.']\n",
      "['row', 'are', 'climbing', 'the', 'rock', 'while', 'the', 'man', 'in', 'red', 'watches', 'and', 'holds', 'the', 'line', '.']\n",
      "- ['are', 'climbing', 'the', 'rock', 'while', 'the', 'man', 'in', 'red', 'watches', 'and', 'holds', 'the', 'line', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'brown', 'dog', 'chases', 'the', 'water', 'from', 'a', 'sp', '##rin', '##kle', '##r', 'on', 'a', 'lawn']\n",
      "- ['a', 'brown', 'dog', 'chases', 'the', 'water', 'from', 'a', 'sp', '##rin', '##kle', '##r', 'on', 'a', 'lawn', '.']\n",
      "['a', 'brown', 'dog', 'chases', 'the', 'water', 'from', 'a', 'sp', '##rin', '##kle', '##r', 'on', 'a', 'lawn', '.']\n",
      "- ['brown', 'dog', 'chases', 'the', 'water', 'from', 'a', 'sp', '##rin', '##kle', '##r', 'on', 'a', 'lawn', '.', '[SEP]']\n",
      "['[CLS]', 'large', 'brown', 'dog', 'running', 'away', 'from', 'the', 'sp', '##rin', '##kle', '##r', 'in', 'the', 'grass', '.']\n",
      "- ['large', 'brown', 'dog', 'running', 'away', 'from', 'the', 'sp', '##rin', '##kle', '##r', 'in', 'the', 'grass', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'dog', 'prepares', 'to', 'catch', 'a', 'thrown', 'object', 'in', 'a', 'field', 'with', 'nearby', 'cars', '.']\n",
      "- ['a', 'dog', 'prepares', 'to', 'catch', 'a', 'thrown', 'object', 'in', 'a', 'field', 'with', 'nearby', 'cars', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'white', 'dog', 'is', 'ready', 'to', 'catch', 'a', 'yellow', 'ball', 'flying', 'through', 'the', 'air', '.']\n",
      "- ['a', 'white', 'dog', 'is', 'ready', 'to', 'catch', 'a', 'yellow', 'ball', 'flying', 'through', 'the', 'air', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'white', 'dog', 'is', 'trying', 'to', 'catch', 'a', 'ball', 'in', 'mid', '##air', 'over', 'a', 'grassy']\n",
      "- ['a', 'white', 'dog', 'is', 'trying', 'to', 'catch', 'a', 'ball', 'in', 'mid', '##air', 'over', 'a', 'grassy', 'field']\n",
      "['a', 'white', 'dog', 'is', 'trying', 'to', 'catch', 'a', 'ball', 'in', 'mid', '##air', 'over', 'a', 'grassy', 'field']\n",
      "- ['white', 'dog', 'is', 'trying', 'to', 'catch', 'a', 'ball', 'in', 'mid', '##air', 'over', 'a', 'grassy', 'field', '.']\n",
      "['white', 'dog', 'is', 'trying', 'to', 'catch', 'a', 'ball', 'in', 'mid', '##air', 'over', 'a', 'grassy', 'field', '.']\n",
      "- ['dog', 'is', 'trying', 'to', 'catch', 'a', 'ball', 'in', 'mid', '##air', 'over', 'a', 'grassy', 'field', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'couple', 'and', 'an', 'infant', ',', 'being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to']\n",
      "- ['a', 'couple', 'and', 'an', 'infant', ',', 'being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a']\n",
      "['a', 'couple', 'and', 'an', 'infant', ',', 'being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a']\n",
      "- ['couple', 'and', 'an', 'infant', ',', 'being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond']\n",
      "['couple', 'and', 'an', 'infant', ',', 'being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond']\n",
      "- ['and', 'an', 'infant', ',', 'being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with']\n",
      "['and', 'an', 'infant', ',', 'being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with']\n",
      "- ['an', 'infant', ',', 'being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with', 'a']\n",
      "['an', 'infant', ',', 'being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with', 'a']\n",
      "- ['infant', ',', 'being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with', 'a', 'near']\n",
      "['infant', ',', 'being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with', 'a', 'near']\n",
      "- [',', 'being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with', 'a', 'near', 'by']\n",
      "[',', 'being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with', 'a', 'near', 'by']\n",
      "- ['being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with', 'a', 'near', 'by', 'stroll']\n",
      "['being', 'held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with', 'a', 'near', 'by', 'stroll']\n",
      "- ['held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with', 'a', 'near', 'by', 'stroll', '##er']\n",
      "['held', 'by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with', 'a', 'near', 'by', 'stroll', '##er']\n",
      "- ['by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with', 'a', 'near', 'by', 'stroll', '##er', '.']\n",
      "['by', 'the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with', 'a', 'near', 'by', 'stroll', '##er', '.']\n",
      "- ['the', 'male', ',', 'sitting', 'next', 'to', 'a', 'pond', 'with', 'a', 'near', 'by', 'stroll', '##er', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'man', 'and', 'woman', 'care', 'for', 'an', 'infant', 'along', 'the', 'side', 'of', 'a', 'body', 'of']\n",
      "- ['a', 'man', 'and', 'woman', 'care', 'for', 'an', 'infant', 'along', 'the', 'side', 'of', 'a', 'body', 'of', 'water']\n",
      "['a', 'man', 'and', 'woman', 'care', 'for', 'an', 'infant', 'along', 'the', 'side', 'of', 'a', 'body', 'of', 'water']\n",
      "- ['man', 'and', 'woman', 'care', 'for', 'an', 'infant', 'along', 'the', 'side', 'of', 'a', 'body', 'of', 'water', '.']\n",
      "['man', 'and', 'woman', 'care', 'for', 'an', 'infant', 'along', 'the', 'side', 'of', 'a', 'body', 'of', 'water', '.']\n",
      "- ['and', 'woman', 'care', 'for', 'an', 'infant', 'along', 'the', 'side', 'of', 'a', 'body', 'of', 'water', '.', '[SEP]']\n",
      "['[CLS]', 'an', 'ice', 'climb', '##er', 'in', 'a', 'blue', 'jacket', 'and', 'black', 'pants', 'is', 'scaling', 'a', 'frozen']\n",
      "- ['an', 'ice', 'climb', '##er', 'in', 'a', 'blue', 'jacket', 'and', 'black', 'pants', 'is', 'scaling', 'a', 'frozen', 'ice']\n",
      "['an', 'ice', 'climb', '##er', 'in', 'a', 'blue', 'jacket', 'and', 'black', 'pants', 'is', 'scaling', 'a', 'frozen', 'ice']\n",
      "- ['ice', 'climb', '##er', 'in', 'a', 'blue', 'jacket', 'and', 'black', 'pants', 'is', 'scaling', 'a', 'frozen', 'ice', 'wall']\n",
      "['ice', 'climb', '##er', 'in', 'a', 'blue', 'jacket', 'and', 'black', 'pants', 'is', 'scaling', 'a', 'frozen', 'ice', 'wall']\n",
      "- ['climb', '##er', 'in', 'a', 'blue', 'jacket', 'and', 'black', 'pants', 'is', 'scaling', 'a', 'frozen', 'ice', 'wall', '.']\n",
      "['climb', '##er', 'in', 'a', 'blue', 'jacket', 'and', 'black', 'pants', 'is', 'scaling', 'a', 'frozen', 'ice', 'wall', '.']\n",
      "- ['##er', 'in', 'a', 'blue', 'jacket', 'and', 'black', 'pants', 'is', 'scaling', 'a', 'frozen', 'ice', 'wall', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'black', 'dog', 'carries', 'a', 'green', 'toy', 'in', 'his', 'mouth', 'as', 'he', 'walks', 'through', 'the']\n",
      "- ['a', 'black', 'dog', 'carries', 'a', 'green', 'toy', 'in', 'his', 'mouth', 'as', 'he', 'walks', 'through', 'the', 'grass']\n",
      "['a', 'black', 'dog', 'carries', 'a', 'green', 'toy', 'in', 'his', 'mouth', 'as', 'he', 'walks', 'through', 'the', 'grass']\n",
      "- ['black', 'dog', 'carries', 'a', 'green', 'toy', 'in', 'his', 'mouth', 'as', 'he', 'walks', 'through', 'the', 'grass', '.']\n",
      "['black', 'dog', 'carries', 'a', 'green', 'toy', 'in', 'his', 'mouth', 'as', 'he', 'walks', 'through', 'the', 'grass', '.']\n",
      "- ['dog', 'carries', 'a', 'green', 'toy', 'in', 'his', 'mouth', 'as', 'he', 'walks', 'through', 'the', 'grass', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'man', 'and', 'a', 'little', 'boy', 'in', 'blue', 'life', 'jackets', 'are', 'rowing', 'a', 'yellow', 'canoe']\n",
      "- ['a', 'man', 'and', 'a', 'little', 'boy', 'in', 'blue', 'life', 'jackets', 'are', 'rowing', 'a', 'yellow', 'canoe', '.']\n",
      "['a', 'man', 'and', 'a', 'little', 'boy', 'in', 'blue', 'life', 'jackets', 'are', 'rowing', 'a', 'yellow', 'canoe', '.']\n",
      "- ['man', 'and', 'a', 'little', 'boy', 'in', 'blue', 'life', 'jackets', 'are', 'rowing', 'a', 'yellow', 'canoe', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'black', 'dog', 'and', 'a', 'brown', 'dog', 'are', 'jumping', 'up', 'to', 'catch', 'a', 'red', 'toy']\n",
      "- ['a', 'black', 'dog', 'and', 'a', 'brown', 'dog', 'are', 'jumping', 'up', 'to', 'catch', 'a', 'red', 'toy', '.']\n",
      "['a', 'black', 'dog', 'and', 'a', 'brown', 'dog', 'are', 'jumping', 'up', 'to', 'catch', 'a', 'red', 'toy', '.']\n",
      "- ['black', 'dog', 'and', 'a', 'brown', 'dog', 'are', 'jumping', 'up', 'to', 'catch', 'a', 'red', 'toy', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'black', 'dog', 'and', 'a', 'brown', 'dog', 'play', 'with', 'a', 'red', 'toy', 'on', 'a', 'courtyard']\n",
      "- ['a', 'black', 'dog', 'and', 'a', 'brown', 'dog', 'play', 'with', 'a', 'red', 'toy', 'on', 'a', 'courtyard', '.']\n",
      "['a', 'black', 'dog', 'and', 'a', 'brown', 'dog', 'play', 'with', 'a', 'red', 'toy', 'on', 'a', 'courtyard', '.']\n",
      "- ['black', 'dog', 'and', 'a', 'brown', 'dog', 'play', 'with', 'a', 'red', 'toy', 'on', 'a', 'courtyard', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'brown', 'and', 'black', 'lab', 'are', 'outside', 'and', 'the', 'black', 'lab', 'is', 'catching', 'a', 'toy']\n",
      "- ['a', 'brown', 'and', 'black', 'lab', 'are', 'outside', 'and', 'the', 'black', 'lab', 'is', 'catching', 'a', 'toy', 'in']\n",
      "['a', 'brown', 'and', 'black', 'lab', 'are', 'outside', 'and', 'the', 'black', 'lab', 'is', 'catching', 'a', 'toy', 'in']\n",
      "- ['brown', 'and', 'black', 'lab', 'are', 'outside', 'and', 'the', 'black', 'lab', 'is', 'catching', 'a', 'toy', 'in', 'its']\n",
      "['brown', 'and', 'black', 'lab', 'are', 'outside', 'and', 'the', 'black', 'lab', 'is', 'catching', 'a', 'toy', 'in', 'its']\n",
      "- ['and', 'black', 'lab', 'are', 'outside', 'and', 'the', 'black', 'lab', 'is', 'catching', 'a', 'toy', 'in', 'its', 'mouth']\n",
      "['and', 'black', 'lab', 'are', 'outside', 'and', 'the', 'black', 'lab', 'is', 'catching', 'a', 'toy', 'in', 'its', 'mouth']\n",
      "- ['black', 'lab', 'are', 'outside', 'and', 'the', 'black', 'lab', 'is', 'catching', 'a', 'toy', 'in', 'its', 'mouth', '.']\n",
      "['black', 'lab', 'are', 'outside', 'and', 'the', 'black', 'lab', 'is', 'catching', 'a', 'toy', 'in', 'its', 'mouth', '.']\n",
      "- ['lab', 'are', 'outside', 'and', 'the', 'black', 'lab', 'is', 'catching', 'a', 'toy', 'in', 'its', 'mouth', '.', '[SEP]']\n",
      "['[CLS]', 'the', 'chocolate', 'lab', 'jumps', 'too', 'late', 'to', 'get', 'the', 'toy', 'as', 'the', 'black', 'lab', 'captures']\n",
      "- ['the', 'chocolate', 'lab', 'jumps', 'too', 'late', 'to', 'get', 'the', 'toy', 'as', 'the', 'black', 'lab', 'captures', 'it']\n",
      "['the', 'chocolate', 'lab', 'jumps', 'too', 'late', 'to', 'get', 'the', 'toy', 'as', 'the', 'black', 'lab', 'captures', 'it']\n",
      "- ['chocolate', 'lab', 'jumps', 'too', 'late', 'to', 'get', 'the', 'toy', 'as', 'the', 'black', 'lab', 'captures', 'it', 'in']\n",
      "['chocolate', 'lab', 'jumps', 'too', 'late', 'to', 'get', 'the', 'toy', 'as', 'the', 'black', 'lab', 'captures', 'it', 'in']\n",
      "- ['lab', 'jumps', 'too', 'late', 'to', 'get', 'the', 'toy', 'as', 'the', 'black', 'lab', 'captures', 'it', 'in', 'the']\n",
      "['lab', 'jumps', 'too', 'late', 'to', 'get', 'the', 'toy', 'as', 'the', 'black', 'lab', 'captures', 'it', 'in', 'the']\n",
      "- ['jumps', 'too', 'late', 'to', 'get', 'the', 'toy', 'as', 'the', 'black', 'lab', 'captures', 'it', 'in', 'the', 'driveway']\n",
      "['jumps', 'too', 'late', 'to', 'get', 'the', 'toy', 'as', 'the', 'black', 'lab', 'captures', 'it', 'in', 'the', 'driveway']\n",
      "- ['too', 'late', 'to', 'get', 'the', 'toy', 'as', 'the', 'black', 'lab', 'captures', 'it', 'in', 'the', 'driveway', '.']\n",
      "['too', 'late', 'to', 'get', 'the', 'toy', 'as', 'the', 'black', 'lab', 'captures', 'it', 'in', 'the', 'driveway', '.']\n",
      "- ['late', 'to', 'get', 'the', 'toy', 'as', 'the', 'black', 'lab', 'captures', 'it', 'in', 'the', 'driveway', '.', '[SEP]']\n",
      "['[CLS]', 'a', 'man', 'in', 'black', 'is', 'sitting', 'next', 'to', 'a', 'modern', 'art', 'structure', 'in', 'front', 'of']\n",
      "- ['a', 'man', 'in', 'black', 'is', 'sitting', 'next', 'to', 'a', 'modern', 'art', 'structure', 'in', 'front', 'of', 'a']\n",
      "['a', 'man', 'in', 'black', 'is', 'sitting', 'next', 'to', 'a', 'modern', 'art', 'structure', 'in', 'front', 'of', 'a']\n",
      "- ['man', 'in', 'black', 'is', 'sitting', 'next', 'to', 'a', 'modern', 'art', 'structure', 'in', 'front', 'of', 'a', 'glass']\n"
     ]
    }
   ],
   "source": [
    "detokenize = tokenizer.convert_ids_to_tokens\n",
    "\n",
    "for images, captions, targets in trainloader:\n",
    "    for i in range(captions.shape[0]):\n",
    "        print(detokenize(captions[i]))\n",
    "        print(\"-\", detokenize(targets[i]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, output_size: int):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.inception_model = models.inception_v3(pretrained=True)\n",
    "        #self.inception_model.fc = torch.nn.Identity()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(1000, output_size)\n",
    "        for name, param in self.inception_model.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    def forward(self, images: torch.Tensor):\n",
    "        features = self.inception_model(images) #[1, 2048]\n",
    "        if isinstance(features, tuple):  # Nếu là tuple\n",
    "            features = features[0] \n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "        features = self.fc(features)\n",
    "        return features\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, hidden_size, input_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, hidden_state: tuple[torch.Tensor, torch.Tensor] = None): \n",
    "        # seq = 1\n",
    "        # features : image_features : [bsz, embed]\n",
    "        # captions : [bsz, seq]\n",
    "        # hidden : [1, bsz, embed]\n",
    "        embeddings = self.embed(captions) # [bsz, seq, embed]\n",
    "        features = features.unsqueeze(1).expand(-1, embeddings.shape[1], -1) # [bsz, seq, embed]\n",
    "        combined = torch.cat((features, embeddings), dim=2) # [bsz, seq, embed*2]\n",
    "        if hidden_state == None:\n",
    "            hidden_state = (\n",
    "                torch.zeros(self.num_layers, captions.shape[0], self.hidden_size).to(captions.device),\n",
    "                torch.zeros(self.num_layers, captions.shape[0], self.hidden_size).to(captions.device)\n",
    "            )\n",
    "        output, hidden = self.lstm(combined, hidden_state) # [bsz, seq, hid]\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output) #[batch_size, seq_len, vocab_size]\n",
    "        return output, hidden\n",
    "class ImageToTextModel(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
    "        self.encoder: EncoderCNN = encoder\n",
    "        self.decoder: DecoderRNN = decoder\n",
    "    def forward(self, images: torch.Tensor, captions: torch.Tensor):\n",
    "        bsz = images.shape[0]\n",
    "        hidden_state: tuple[torch.Tensor, torch.Tensor] = None\n",
    "        features = self.encoder(images)\n",
    "        seq_predicted = []\n",
    "        seq_predicted.append(torch.zeros((bsz, self.decoder.vocab_size), dtype=torch.float32).to(device))\n",
    "        decoder_input = captions[:, 0]\n",
    "        seq_length = captions.shape[1]\n",
    "        for di in range(1, seq_length):\n",
    "            output_decoder, hidden_state = self.decoder(features, decoder_input, hidden_state)\n",
    "            decoder_input = captions[:, di]\n",
    "            seq_predicted.append(output_decoder)\n",
    "        return torch.tensor(seq_predicted)\n",
    "    def predict(self, images: torch.Tensor, captions: torch.Tensor, predict_length: int):\n",
    "        hidden_state: tuple[torch.Tensor, torch.Tensor] = None\n",
    "        features = self.encoder(images)\n",
    "        seq_predicted = []\n",
    "        decoder_input = captions[:, 0]\n",
    "        seq_length = captions.shape[1]\n",
    "        for di in range(1, seq_length):\n",
    "            output_decoder, hidden_state = self.decoder(features, decoder_input, hidden_state)\n",
    "            decoder_input = captions[:, di]\n",
    "        for di in range(1, predict_length):\n",
    "            output_decoder, hidden_state = self.decoder(features, decoder_input, hidden_state)\n",
    "            seq_predicted.append(output_decoder)\n",
    "            decoder_input = output_decoder.argmax(1)\n",
    "        return torch.tensor(seq_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      " 10%|█         | 1/10 [00:27<04:05, 27.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Test loss : 6.559357643127441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:55<03:40, 27.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Test loss : 4.8456621170043945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:22<03:13, 27.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Test loss : 4.175093650817871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:50<02:46, 27.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Test loss : 3.698627471923828\n"
     ]
    }
   ],
   "source": [
    "image_size = 128\n",
    "encoder = EncoderCNN(\n",
    "    output_size=image_size\n",
    ")\n",
    "decoder = DecoderRNN(\n",
    "    embed_size=16,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,\n",
    "    input_size=image_size,\n",
    "    num_layers=1\n",
    ")\n",
    "image_to_text_model = ImageToTextModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder\n",
    ")\n",
    "optimizer = torch.optim.Adam(image_to_text_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 10\n",
    "image_to_text_model.to(device)\n",
    "image_to_text_model.train()\n",
    "detokenize = tokenizer.convert_ids_to_tokens\n",
    "\n",
    "for epoch in tqdm.trange(num_epochs):\n",
    "    i = 0\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    for images, captions in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = targets.to(device)\n",
    "        encoder_output = encoder(images)\n",
    "        # captions = torch.clamp(captions, 1, 1e5).long()\n",
    "        decoder_output, output_hidden = decoder(encoder_output, captions)\n",
    "        decoder_output: torch.Tensor\n",
    "        # decoder_output = torch.clamp(decoder_output, 1e-2, 1e2)\n",
    "\n",
    "        loss: torch.Tensor = criterion(decoder_output.view(-1, decoder_output.shape[2]), captions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        count += 1 \n",
    "        i+=1\n",
    "        # print(\"Finish batch\")\n",
    "        # print(f\"{loss:.5f} | {i}/{len(trainloader)}\")\n",
    "    print(f\"Epoch {epoch+1} | Test loss : {total_loss/count}\")\n",
    "    # random : log(1/30k) ~ 10.31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trees\n",
      "['vest', 'in', 'on', '##ing', '##ing', 'as', 'while', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'trees']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "def interactive_test(\n",
    "        encoder: nn.Module,\n",
    "        decoder: nn.Module,\n",
    "        image_path: str,\n",
    "        text: str\n",
    "    ):\n",
    "    tokens: list[int] = tokenizer.encode(text)\n",
    "    tokens.pop(-1)\n",
    "    while(len(tokens) < SEQ_LENGTH):\n",
    "        tokens = [0] + tokens\n",
    "    tokens: torch.Tensor = torch.tensor(tokens)\n",
    "    images = torch.Tensor(image_transforms(Image.open(image_path).convert(\"RGB\")))\n",
    "    images = images.unsqueeze(0)\n",
    "    tokens = tokens.unsqueeze(0)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    images = images.to(device)\n",
    "    tokens = tokens.to(device)\n",
    "    encoder_output = encoder(images)\n",
    "        # captions = torch.clamp(captions, 1, 1e5).long()\n",
    "    decoder_output, output_hidden = decoder(encoder_output, tokens)\n",
    "    decoder_output: torch.Tensor\n",
    "    # print(images.shape)\n",
    "    # print(tokens.shape)\n",
    "    # print(encoder_output.shape)\n",
    "    # decoder_output, output_hidden = decoder(encoder_output, tokens)\n",
    "    # decoder_output: torch.Tensor\n",
    "        # print(decoder_output.shape)\n",
    "        # # decoder_output = decoder_output.squeeze(1)\n",
    "        # print(decoder_output.shape)\n",
    "        # print(targets.shape)\n",
    "    target_logits = decoder_output[:, -1, :]\n",
    "    predicts = decoder_output.argmax(2)\n",
    "    predicted_token = target_logits.argmax()\n",
    "    return predicted_token, tokens, predicts\n",
    "\n",
    "image_path = \"Flickr8k/Flicker8k_Dataset/44856031_0d82c2c7d1.jpg\"\n",
    "# image_path = \"Flickr8k/Flicker8k_Dataset/110595925_f3395c8bd6.jpg\"\n",
    "text = \"\"\n",
    "predict_token, captions, predicts = interactive_test(encoder, decoder, image_path, text)\n",
    "# print(predicts, captions)\n",
    "predict_token = predict_token.item()\n",
    "predicts = predicts[0]\n",
    "# print(predicts)\n",
    "# print(captions)\n",
    "detokenize = tokenizer.convert_ids_to_tokens\n",
    "print(detokenize(predict_token))\n",
    "print(detokenize(predicts))\n",
    "\n",
    "# for i in range(min(len(predicts), captions.shape[0])):\n",
    "#     print(detokenize(predicts[i]), detokenize(captions[i].item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

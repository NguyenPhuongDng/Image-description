{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format lại code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import math\n",
    "import torch.multiprocessing as mp\n",
    "from transformers import BertTokenizerFast\n",
    "import pickle\n",
    "# Evaluate\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# from nltk.translate.meteor_score import meteor_score\n",
    "# from rogue import Rogue\n",
    "# from pycocoevalcap.cider.cider import Cider\n",
    "# from pycocoevalcap.spice.spice import Spice\n",
    "#end\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "detokenize = tokenizer.convert_ids_to_tokens\n",
    "batch_detokenize = tokenizer.batch_decode\n",
    "BATCH_SIZE = 64 \n",
    "#64 : 3.1GB VRAM b16 (3.0 Dedicated | 0.1 Shared)\n",
    "device = 'cuda'\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Dùng cho ImageNet\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loading 10.01 % \n",
      "Image loading 20.02 % \n",
      "Image loading 30.03 % \n",
      "Image loading 40.04 % \n",
      "Image loading 50.06 % \n",
      "Image loading 60.07 % \n",
      "Image loading 70.08 % \n",
      "Image loading 80.09 % \n",
      "Image loading 90.10 % \n",
      "Image loading 100 % \n"
     ]
    }
   ],
   "source": [
    "def load_image(image_folder_path: str): # DataLoader trên Jupyternotebook ko xử lý đa luồng đc nên đành phải load hết vô\n",
    "    result = {}\n",
    "    file_counts = len(os.listdir(image_folder_path))\n",
    "    progress = 0\n",
    "    last_log = 0\n",
    "    count = 0\n",
    "    for file_name in os.listdir(image_folder_path):\n",
    "        file_path = os.path.join(image_folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            id = os.path.splitext(os.path.basename(file_name))[0]\n",
    "            if id in result: continue\n",
    "            with Image.open(file_path).convert(\"RGB\") as img:\n",
    "                image = image_transforms(img)\n",
    "                result[id] = torch.Tensor(image)\n",
    "            count += 1\n",
    "            progress = count / file_counts * 100\n",
    "            if progress - last_log >= 10:\n",
    "                last_log = progress\n",
    "                print(f\"Image loading {progress:.2f} % \")\n",
    "    if last_log != 100:\n",
    "        print(f\"Image loading 100 % \")\n",
    "    return result\n",
    "def process_data(image_dict: dict[str, torch.Tensor], processed_data_path: str):\n",
    "    result: list[tuple[torch.Tensor, torch.Tensor]] = []\n",
    "    with open(processed_data_path, 'rb') as file:\n",
    "        processed_data = pickle.load(file)\n",
    "    for image_id, caption in processed_data:\n",
    "        caption = torch.tensor(caption)\n",
    "        image = image_dict[image_id]\n",
    "        result.append((image, caption))\n",
    "    return result\n",
    "def get_train_test_loader(batch_size: int, n_wokers: int = 2):\n",
    "    image_path = \"../Flickr8k/Flicker8k_Dataset\"\n",
    "    train_path = \"../Data_bert/train_set_bert.pkl\"\n",
    "    test_path = \"../Data_bert/test_set_bert.pkl\"\n",
    "    image_dict: dict[str, torch.Tensor] = load_image(image_path)\n",
    "    train_data = process_data(image_dict, train_path)\n",
    "    test_data = process_data(image_dict, test_path)\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, num_workers=n_wokers, shuffle=True)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, num_workers=n_wokers, shuffle=False)\n",
    "    return trainloader, testloader\n",
    "trainloader, testloader = get_train_test_loader(BATCH_SIZE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, captions in trainloader:\n",
    "#     for i in range(min(4, captions.shape[0])):\n",
    "#         print(images[i][:,100,100])\n",
    "#         print(detokenize(captions[i]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, output_size: int):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.inception_model = models.inception_v3(pretrained=True)\n",
    "        #self.inception_model.fc = torch.nn.Identity()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(1000, output_size)\n",
    "        for name, param in self.inception_model.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    def forward(self, images: torch.Tensor):\n",
    "        features = self.inception_model(images) #[1, 2048]\n",
    "        if isinstance(features, tuple):  # Nếu là tuple\n",
    "            features = features[0] \n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "        features = self.fc(features)\n",
    "        return features\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, hidden_size, input_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_hidden = False\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, hidden_state: tuple[torch.Tensor, torch.Tensor] = None): \n",
    "        # Constant : seq = 1\n",
    "        # features : [bsz, img_sz]\n",
    "        # captions : [bsz, seq]\n",
    "        # hidden : [num_layers, bsz, hidden]\n",
    "        embeddings = self.embed(captions) # [bsz, seq, embed]\n",
    "        features = features.unsqueeze(1).expand(-1, embeddings.shape[1], -1) # [bsz, seq, embed]\n",
    "        combined = torch.cat((features, embeddings), dim=2) # [bsz, seq, img_sz + embed]\n",
    "        # hidden_state : [num_layers, seq, hid] * 2\n",
    "        if self.use_hidden:\n",
    "            output, hidden_state = self.lstm(combined, hidden_state)\n",
    "        else:\n",
    "            output, hidden_state = self.lstm(combined)\n",
    "        # output : [bsz, seq, vocab_size]\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden_state\n",
    "class ImageToTextModel(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
    "        super(ImageToTextModel, self).__init__()\n",
    "        self.encoder: EncoderCNN = encoder\n",
    "        self.decoder: DecoderRNN = decoder\n",
    "    def forward(self, images: torch.Tensor, captions: torch.Tensor):\n",
    "        # Constant : SEQ_LENGTH = 46, seq = 1\n",
    "        # images: [bsz, 3, raw_image_width, raw_image_height]\n",
    "        # captions: [bsz, SEQ_LENGTH]\n",
    "        bsz = images.shape[0]\n",
    "        if self.decoder.use_hidden:\n",
    "            hidden_state: tuple[torch.Tensor, torch.Tensor] = None\n",
    "        features = self.encoder(images)\n",
    "        # features: [bsz, img_sz]\n",
    "        seq_predicted = []\n",
    "        seq_predicted.append(torch.zeros((bsz, self.decoder.vocab_size), dtype=torch.float32).unsqueeze(1).to(device))\n",
    "        # seq_predicted : [predict_length, seq, vocab]\n",
    "        decoder_input = captions[:, 0].unsqueeze(1)\n",
    "        # decoder_input : [bsz, seq]\n",
    "        seq_length = captions.shape[1]\n",
    "        for di in range(1, seq_length):\n",
    "            if self.decoder.use_hidden:\n",
    "                output_decoder, hidden_state = self.decoder(features, decoder_input, hidden_state)\n",
    "            else:\n",
    "                output_decoder, hidden_state = self.decoder(features, decoder_input)\n",
    "            # ouput_decoder: [bsz, seq, vocab]\n",
    "            # hidden_state: [num_layers, bsz, hidden_size] * 2\n",
    "            decoder_input = captions[:, di].unsqueeze(1)\n",
    "            # decoder_input : [bsz, seq]\n",
    "            seq_predicted.append(output_decoder)\n",
    "        return torch.cat(seq_predicted, dim=1)\n",
    "    def predict(self, images: torch.Tensor, captions: torch.Tensor, predict_length: int):\n",
    "        bsz = images.shape[0]\n",
    "        hidden_state: tuple[torch.Tensor, torch.Tensor] = None\n",
    "        features = self.encoder(images)\n",
    "        seq_predicted = []\n",
    "        seq_predicted.append(torch.zeros((bsz, self.decoder.vocab_size), dtype=torch.float32).unsqueeze(1).to(device))\n",
    "        decoder_input = captions\n",
    "        for _ in range(predict_length-1):\n",
    "            output_decoder, hidden_state = self.decoder(features, decoder_input, hidden_state)\n",
    "            seq_predicted.append(output_decoder)\n",
    "            decoder_input = output_decoder.argmax(2)\n",
    "        return torch.cat(seq_predicted, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model : ImageToTextModel, dataloader : DataLoader, lossf : callable, optimizer : torch.optim.Optimizer, mixed: bool, device: str):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    count = 0\n",
    "    for images, captions in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        images: torch.Tensor = images.to(device)\n",
    "        captions: torch.Tensor = captions.to(device)\n",
    "        if mixed:\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                outputs: torch.Tensor = model(images, captions)\n",
    "                loss: torch.Tensor = lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        else:\n",
    "            outputs: torch.Tensor = model(images, captions)\n",
    "            loss: torch.Tensor = lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        count += 1 \n",
    "    return epoch_loss / count\n",
    "def compute_metrics(predict: torch.Tensor, caption: torch.Tensor): # Quên chưa xóa token đầu ...\n",
    "    # Accuracy\n",
    "    is_correct = 1 if predict[1] == caption[1] else 0\n",
    "    caption: list[str] = detokenize(caption)\n",
    "    predict: list[str] = detokenize(predict)\n",
    "    while (caption[-1] == \"[PAD]\"):\n",
    "        caption.pop()\n",
    "    while (predict[-1] == \"[PAD]\"):\n",
    "        predict.pop()\n",
    "    # Bleu\n",
    "    reference = [caption]\n",
    "    # print(caption, predict)\n",
    "    bleu_1 = sentence_bleu(reference, predict, weights=(1, 0, 0, 0))\n",
    "    bleu_2 = sentence_bleu(reference, predict, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu_3 = sentence_bleu(reference, predict, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu_4 = sentence_bleu(reference, predict)\n",
    "    # Rogue\n",
    "    # rogue = Rogue()\n",
    "    # scores = rogue.get_scores(' '.join(predicts), ' '.join(captions))\n",
    "    # Meteor\n",
    "    # meteor_score_ = meteor_score([' '.join(captions)], ' '.join(predicts))\n",
    "    # Cider, spice\n",
    "    # cider_scorer = Cider()\n",
    "    # spice_scorer = Spice()\n",
    "    # cider_score, _ = cider_scorer.compute_score({0 : [' '.join(caption)]}, {0 : [' '.join(predict)]})\n",
    "    # spice_score, _ = spice_scorer.compute_score({0 : [' '.join(captions)]}, {0 : [' '.join(predicts)]})\n",
    "    return {\n",
    "        \"accuracy\" : is_correct,\n",
    "        \"bleu_1\" : bleu_1,\n",
    "        \"bleu_2\" : bleu_2,\n",
    "        \"bleu_3\" : bleu_3,\n",
    "        \"bleu_4\" : bleu_4,\n",
    "        # \"rogue-1\" : scores[\"rogue-1\"][\"f\"],\n",
    "        # \"rogue-2\" : scores[\"rogue-2\"][\"f\"],\n",
    "        # \"rogue-l\" : scores[\"rogue-l\"][\"f\"],\n",
    "        # \"meteor\" : meteor_score_,\n",
    "        # \"cider\" : cider_score,\n",
    "        # \"spice\" : spice_score\n",
    "    }\n",
    "def test(model : ImageToTextModel, dataloader : DataLoader, lossf : callable, mixed: bool, device: str):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    total_count = 0\n",
    "    epoch_metrics = {'loss' : 0}\n",
    "    input = torch.tensor([101]).to(device)\n",
    "    for images, captions in dataloader:\n",
    "        images: torch.Tensor = images.to(device)\n",
    "        captions: torch.Tensor = captions.to(device)\n",
    "        bsz = images.shape[0]\n",
    "        inputs = input.unsqueeze(1).expand((bsz, 1))\n",
    "        if mixed:\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                outputs: torch.Tensor = model.predict(images, inputs, captions.shape[1])\n",
    "                loss: torch.Tensor= lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        else:\n",
    "            outputs: torch.Tensor  = model.predict(images, inputs, captions.shape[1])\n",
    "            loss: torch.Tensor= lossf(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        predicts = outputs.argmax(2)\n",
    "        count += 1\n",
    "        total_count += bsz\n",
    "        epoch_metrics['loss'] += loss.item()\n",
    "        for i in range(bsz):\n",
    "            metrics = compute_metrics(predicts[i], captions[i])\n",
    "            for key in metrics:\n",
    "                if key not in epoch_metrics:\n",
    "                    epoch_metrics[key] = metrics[key]\n",
    "                else:\n",
    "                    epoch_metrics[key] += metrics[key]\n",
    "    for key in epoch_metrics:\n",
    "        if key in ['loss']:\n",
    "            epoch_metrics[key] /= count\n",
    "        else:\n",
    "            epoch_metrics[key] /= total_count\n",
    "    return epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 10.3357 | Test loss : 10.3010 | Train time : 3.26 s | Lr : 0.00100000\n",
      "{'loss': 10.301046371459961, 'accuracy': 0.0, 'bleu_1': 0.0, 'bleu_2': 0.0, 'bleu_3': 0.0, 'bleu_4': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "image_size = 256\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "encoder = EncoderCNN(\n",
    "    output_size=image_size\n",
    ")\n",
    "decoder = DecoderRNN(\n",
    "    embed_size=embed_size,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=hidden_size,\n",
    "    input_size=image_size,\n",
    "    num_layers=1\n",
    ")\n",
    "decoder.use_hidden = False\n",
    "image_to_text_model = ImageToTextModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder\n",
    ")\n",
    "loss_func= nn.CrossEntropyLoss(ignore_index=0)\n",
    "model_path, train_log = util.train_eval(\n",
    "    trainloader=trainloader,\n",
    "    testloader=testloader,\n",
    "    model=image_to_text_model,\n",
    "    train_func=train,\n",
    "    test_func=test,\n",
    "    lossf=loss_func,\n",
    "    num_epochs=50,\n",
    "    lr=2.5e-3,\n",
    "    gamma=0.95,\n",
    "    log_step=1,\n",
    "    warmup_nepochs=10,\n",
    "    warmup_lr=1e-3,\n",
    "    warmup_gamma=1.1,\n",
    "    save=True,\n",
    "    mixed_train = True,\n",
    "    mixed_eval = False,\n",
    "    save_path=\"checkpoint/teset_model\",\n",
    "    optimizer_type=torch.optim.Adam,\n",
    "    device=device,\n",
    "    metadata_extra={\n",
    "        \"batch_size\" : BATCH_SIZE,\n",
    "        \"dataset_name\" : \"Flickr8k\",\n",
    "        \"use_hidden\" : decoder.use_hidden\n",
    "    },\n",
    "    log_metric=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:06<05:02,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 10.3144 | Test loss : 10.2732 | Train time : 3.30 s | Lr : 0.00100000\n",
      "{'loss': 10.273191452026367, 'accuracy': 0.3, 'bleu_1': 0.019565217391304346, 'bleu_2': 1.4428752331563675e-155, 'bleu_3': 1.4966750252888628e-204, 'bleu_4': 3.98358021083051e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:12<04:51,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 10.1874 | Test loss : 10.1764 | Train time : 3.03 s | Lr : 0.00110000\n",
      "{'loss': 10.176424980163574, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:18<04:43,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 10.0034 | Test loss : 10.0100 | Train time : 3.02 s | Lr : 0.00121000\n",
      "{'loss': 10.010049819946289, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:24<04:41,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 9.6733 | Test loss : 9.7731 | Train time : 3.12 s | Lr : 0.00133100\n",
      "{'loss': 9.773061752319336, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:30<04:32,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 9.2667 | Test loss : 9.5097 | Train time : 3.07 s | Lr : 0.00146410\n",
      "{'loss': 9.509742736816406, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:36<04:22,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 8.8059 | Test loss : 9.1889 | Train time : 2.90 s | Lr : 0.00161051\n",
      "{'loss': 9.188897132873535, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:41<04:12,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 8.3497 | Test loss : 8.8821 | Train time : 2.87 s | Lr : 0.00177156\n",
      "{'loss': 8.88210678100586, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:47<04:07,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 7.8274 | Test loss : 8.5995 | Train time : 3.03 s | Lr : 0.00194872\n",
      "{'loss': 8.599486351013184, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:53<04:02,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 7.3260 | Test loss : 8.2627 | Train time : 3.03 s | Lr : 0.00214359\n",
      "{'loss': 8.262710571289062, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:59<03:57,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 6.7026 | Test loss : 7.9441 | Train time : 2.95 s | Lr : 0.00235795\n",
      "{'loss': 7.944087028503418, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [01:05<03:51,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 6.1240 | Test loss : 7.6882 | Train time : 3.11 s | Lr : 0.00259374\n",
      "{'loss': 7.688172817230225, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [01:11<03:44,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 5.5626 | Test loss : 7.5512 | Train time : 2.95 s | Lr : 0.00285312\n",
      "{'loss': 7.551210880279541, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [01:17<03:39,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 5.0281 | Test loss : 7.5918 | Train time : 3.10 s | Lr : 0.00313843\n",
      "{'loss': 7.591808319091797, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [01:23<03:37,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.6959 | Test loss : 7.8316 | Train time : 3.04 s | Lr : 0.00345227\n",
      "{'loss': 7.831576824188232, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [01:29<03:32,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.4431 | Test loss : 8.1356 | Train time : 3.03 s | Lr : 0.00379750\n",
      "{'loss': 8.135635375976562, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [01:35<03:26,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.3365 | Test loss : 8.5048 | Train time : 3.02 s | Lr : 0.00400000\n",
      "{'loss': 8.50479507446289, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [01:42<03:21,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2177 | Test loss : 8.7724 | Train time : 3.19 s | Lr : 0.00360000\n",
      "{'loss': 8.772432327270508, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [01:48<03:16,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2735 | Test loss : 9.0522 | Train time : 3.10 s | Lr : 0.00324000\n",
      "{'loss': 9.052202224731445, 'accuracy': 0.0, 'bleu_1': 0.021739130434782605, 'bleu_2': 2.199344694155892e-155, 'bleu_3': 2.514995661874996e-204, 'bleu_4': 6.995501686664744e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [01:54<03:08,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2747 | Test loss : 9.2094 | Train time : 3.09 s | Lr : 0.00291600\n",
      "{'loss': 9.209364891052246, 'accuracy': 0.0, 'bleu_1': 0.021739130434782605, 'bleu_2': 2.199344694155892e-155, 'bleu_3': 2.514995661874996e-204, 'bleu_4': 6.995501686664744e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [02:00<03:04,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.1936 | Test loss : 9.3552 | Train time : 3.23 s | Lr : 0.00262440\n",
      "{'loss': 9.355242729187012, 'accuracy': 0.0, 'bleu_1': 0.021739130434782605, 'bleu_2': 2.199344694155892e-155, 'bleu_3': 2.514995661874996e-204, 'bleu_4': 6.995501686664744e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [02:06<02:55,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2050 | Test loss : 9.5152 | Train time : 2.97 s | Lr : 0.00236196\n",
      "{'loss': 9.515209197998047, 'accuracy': 0.0, 'bleu_1': 0.021739130434782605, 'bleu_2': 2.199344694155892e-155, 'bleu_3': 2.514995661874996e-204, 'bleu_4': 6.995501686664744e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [02:12<02:49,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2907 | Test loss : 9.6038 | Train time : 3.05 s | Lr : 0.00212576\n",
      "{'loss': 9.603788375854492, 'accuracy': 0.0, 'bleu_1': 0.021739130434782605, 'bleu_2': 2.199344694155892e-155, 'bleu_3': 2.514995661874996e-204, 'bleu_4': 6.995501686664744e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [02:18<02:42,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2340 | Test loss : 9.7008 | Train time : 3.06 s | Lr : 0.00191319\n",
      "{'loss': 9.70083999633789, 'accuracy': 0.0, 'bleu_1': 0.021739130434782605, 'bleu_2': 2.199344694155892e-155, 'bleu_3': 2.514995661874996e-204, 'bleu_4': 6.995501686664744e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [02:24<02:36,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2024 | Test loss : 9.7346 | Train time : 2.93 s | Lr : 0.00172187\n",
      "{'loss': 9.734627723693848, 'accuracy': 0.0, 'bleu_1': 0.021739130434782605, 'bleu_2': 2.199344694155892e-155, 'bleu_3': 2.514995661874996e-204, 'bleu_4': 6.995501686664744e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [02:30<02:30,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2106 | Test loss : 9.8102 | Train time : 3.13 s | Lr : 0.00154968\n",
      "{'loss': 9.810232162475586, 'accuracy': 0.7, 'bleu_1': 0.03695652173913043, 'bleu_2': 2.8370435746127058e-155, 'bleu_3': 2.9674669272513624e-204, 'bleu_4': 7.922020771156347e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [02:36<02:23,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.1803 | Test loss : 9.8972 | Train time : 3.02 s | Lr : 0.00139471\n",
      "{'loss': 9.897170066833496, 'accuracy': 0.7, 'bleu_1': 0.04782608695652173, 'bleu_2': 3.1755883016109695e-155, 'bleu_3': 3.1845015165594013e-204, 'bleu_4': 8.345673348941911e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [02:42<02:16,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.1607 | Test loss : 9.9441 | Train time : 3.00 s | Lr : 0.00125524\n",
      "{'loss': 9.944064140319824, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [02:48<02:10,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2309 | Test loss : 9.9747 | Train time : 2.94 s | Lr : 0.00112972\n",
      "{'loss': 9.974706649780273, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [02:53<02:03,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.1821 | Test loss : 9.9916 | Train time : 2.92 s | Lr : 0.00101675\n",
      "{'loss': 9.991578102111816, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [03:00<01:59,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.1876 | Test loss : 9.9873 | Train time : 3.04 s | Lr : 0.00091507\n",
      "{'loss': 9.987344741821289, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [03:06<01:53,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2264 | Test loss : 9.9742 | Train time : 3.06 s | Lr : 0.00082356\n",
      "{'loss': 9.974248886108398, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [03:12<01:47,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2001 | Test loss : 9.9770 | Train time : 3.05 s | Lr : 0.00074121\n",
      "{'loss': 9.977022171020508, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [03:18<01:41,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.1882 | Test loss : 9.9868 | Train time : 3.06 s | Lr : 0.00066709\n",
      "{'loss': 9.986806869506836, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [03:24<01:36,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.1701 | Test loss : 9.9968 | Train time : 3.08 s | Lr : 0.00060038\n",
      "{'loss': 9.996774673461914, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [03:30<01:31,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2252 | Test loss : 10.0056 | Train time : 3.23 s | Lr : 0.00054034\n",
      "{'loss': 10.005646705627441, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [03:36<01:25,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2640 | Test loss : 10.0134 | Train time : 3.04 s | Lr : 0.00048631\n",
      "{'loss': 10.013350486755371, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [03:42<01:18,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.1567 | Test loss : 10.0195 | Train time : 3.01 s | Lr : 0.00043768\n",
      "{'loss': 10.019546508789062, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [03:48<01:12,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2176 | Test loss : 10.0253 | Train time : 3.09 s | Lr : 0.00039391\n",
      "{'loss': 10.025252342224121, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [03:54<01:06,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2167 | Test loss : 10.0317 | Train time : 3.14 s | Lr : 0.00035452\n",
      "{'loss': 10.031721115112305, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [04:00<01:00,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2174 | Test loss : 10.0372 | Train time : 2.99 s | Lr : 0.00031907\n",
      "{'loss': 10.03724479675293, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [04:06<00:54,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.1973 | Test loss : 10.0444 | Train time : 3.07 s | Lr : 0.00028716\n",
      "{'loss': 10.044437408447266, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [04:12<00:48,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2064 | Test loss : 10.0514 | Train time : 3.07 s | Lr : 0.00025844\n",
      "{'loss': 10.05142879486084, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [04:18<00:42,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.1750 | Test loss : 10.0564 | Train time : 3.07 s | Lr : 0.00023260\n",
      "{'loss': 10.056373596191406, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [04:24<00:36,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2377 | Test loss : 10.0604 | Train time : 3.14 s | Lr : 0.00020934\n",
      "{'loss': 10.060368537902832, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [04:30<00:30,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.1969 | Test loss : 10.0619 | Train time : 2.98 s | Lr : 0.00018841\n",
      "{'loss': 10.06187629699707, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [04:36<00:23,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2390 | Test loss : 10.0624 | Train time : 3.01 s | Lr : 0.00016956\n",
      "{'loss': 10.062381744384766, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [04:42<00:17,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2027 | Test loss : 10.0618 | Train time : 3.06 s | Lr : 0.00015261\n",
      "{'loss': 10.061808586120605, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [04:48<00:11,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.2162 | Test loss : 10.0611 | Train time : 3.05 s | Lr : 0.00013735\n",
      "{'loss': 10.061114311218262, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [04:54<00:06,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.1819 | Test loss : 10.0608 | Train time : 3.28 s | Lr : 0.00012361\n",
      "{'loss': 10.060831069946289, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:00<00:00,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss : 4.1630 | Test loss : 10.0601 | Train time : 3.04 s | Lr : 0.00011125\n",
      "{'loss': 10.060079574584961, 'accuracy': 0.7, 'bleu_1': 0.02608695652173913, 'bleu_2': 1.973844012052805e-155, 'bleu_3': 2.0643129098604857e-204, 'bleu_4': 5.5150404173765435e-232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderCNN(\n",
    "    output_size=image_size\n",
    ")\n",
    "decoder = DecoderRNN(\n",
    "    embed_size=embed_size,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=hidden_size,\n",
    "    input_size=image_size,\n",
    "    num_layers=1\n",
    ")\n",
    "decoder.use_hidden = True\n",
    "image_to_text_model = ImageToTextModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder\n",
    ")\n",
    "loss_func= nn.CrossEntropyLoss(ignore_index=0)\n",
    "model_path, train_log = util.train_eval(\n",
    "    trainloader=trainloader,\n",
    "    testloader=testloader,\n",
    "    model=image_to_text_model,\n",
    "    train_func=train,\n",
    "    test_func=test,\n",
    "    lossf=loss_func,\n",
    "    num_epochs=50,\n",
    "    lr=2.5e-3,\n",
    "    gamma=0.95,\n",
    "    log_step=1,\n",
    "    warmup_nepochs=10,\n",
    "    warmup_lr=1e-3,\n",
    "    warmup_gamma=1.1,\n",
    "    save=True,\n",
    "    mixed_train = True,\n",
    "    mixed_eval = False,\n",
    "    save_path=\"checkpoint/teset_model\",\n",
    "    optimizer_type=torch.optim.Adam,\n",
    "    device=device,\n",
    "    metadata_extra={\n",
    "        \"batch_size\" : BATCH_SIZE,\n",
    "        \"dataset_name\" : \"Flickr8k\",\n",
    "        \"use_hidden\" : decoder.use_hidden\n",
    "    },\n",
    "    log_metric=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

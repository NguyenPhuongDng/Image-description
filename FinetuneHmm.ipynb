{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dùng tạm mô hình tự code để cho nó đúng đầu vào đầu ra đã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from pickle import dump, load\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "BATCH_SIZE = 256 # 3.8GB VRAM f32 (3.6 Dedicated + 0.2 Shared)\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((324, 324)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Dùng cho ImageNet\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "1000\n",
      "30000\n",
      "5000\n",
      "torch.Size([3, 324, 324]) torch.Size([46])\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "data_folder_path = \"Flickr8k/Flicker8k_Dataset\"\n",
    "train_dataset = util.Flickr8kDataset(\n",
    "    data_folder_path,\n",
    "    \"Data_bert/train_set_bert.pkl\",\n",
    "    image_transforms,\n",
    "    device=device\n",
    ")\n",
    "trainloader = train_dataset.get_dataloader(batch_size=BATCH_SIZE, num_workers=2, shuffle=False)\n",
    "test_dataset = util.Flickr8kDataset(\n",
    "    data_folder_path,\n",
    "    \"Data_bert/test_set_bert.pkl\",\n",
    "    image_transforms,\n",
    "    device=device\n",
    ")\n",
    "testloader = test_dataset.get_dataloader(batch_size=BATCH_SIZE, num_workers=2, shuffle=False)\n",
    "print(len(train_dataset)) # :)) sao có tận 30k ids ảnh trong train_set_bert.pkl, một ảnh có nhiều caption à ?\n",
    "print(len(test_dataset)) \n",
    "sample_image, sample_caption = test_dataset[0]\n",
    "print(sample_image.shape, sample_caption.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.CNN = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=3), # 324 * 324\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=3), # 108 * 108\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1), # 36 * 36\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3), # 12 * 12\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1), # 10 * 10\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1), # 8* 8 ?? 7 * 7\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, out_features=512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "    def forward(self, images: torch.Tensor):\n",
    "        return self.CNN(images)\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, hidden_size, input_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, hiden_state: tuple[torch.Tensor, torch.Tensor]): \n",
    "        # features : image_features : [bsz, embed]\n",
    "        # captions : [bsz, seq]\n",
    "        # hidden : [1, bsz, embed]\n",
    "        # print(\"Input dim\")\n",
    "        # print(features.shape)\n",
    "        # print(captions.shape)\n",
    "        # print(hiden_state[0].shape)\n",
    "        embeddings = self.embed(captions) # [bsz, seq, embed]\n",
    "        features = features.unsqueeze(1).expand(-1, embeddings.shape[1], -1) # [bsz, seq, embed]\n",
    "        combined = torch.cat((embeddings, features), dim=2) # [bsz, seq, embed*2]\n",
    "        # print(combined.shape, hiden_state[0].shape, hiden_state[1].shape)\n",
    "        output, hidden = self.lstm(combined, hiden_state) # [bsz, seq, hid]\n",
    "        output = self.relu(output)\n",
    "        output = self.linear(output) #[batch_size, seq_len, vocab_size]\n",
    "        # print(\"End\")\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Test Encoder\n",
    "# encoder = EncoderCNN()\n",
    "# encoder.eval()\n",
    "# output_en = encoder(train_dataset[0][0].unsqueeze(0))\n",
    "# print(output_en.shape)\n",
    "# print(tokenizer.convert_ids_to_tokens(101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test decoder\n",
    "# vocab_size = tokenizer.vocab_size\n",
    "# embed_size = 16\n",
    "# num_layers = 1\n",
    "# hidden_size = 128\n",
    "# hiden = torch.zeros(1, 1, hidden_size)\n",
    "# cell = torch.zeros(1, 1, hidden_size)\n",
    "# hiden_state = (hiden, cell)\n",
    "# hiden_state[0].shape\n",
    "# decoder = DecoderRNN(embed_size, vocab_size, hidden_size, 256, num_layers)\n",
    "# decoder.eval()\n",
    "# state = torch.tensor([101, 102, 103]).unsqueeze(0)\n",
    "# # print(output_en.shape)\n",
    "# # print(state.shape)\n",
    "# # print(hiden_state[0].shape, hiden_state[1].shape)\n",
    "# outputs, hidden = decoder(output_en, state, hiden_state)\n",
    "# # print(outputs.shape)\n",
    "# # print(hidden[0].shape)\n",
    "# # print(outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_batch(\n",
    "        encoder: nn.Module,\n",
    "        decoder: nn.Module,\n",
    "        images: torch.Tensor,\n",
    "        captions: torch.Tensor,\n",
    "        encoder_optimizer: torch.optim.Optimizer,\n",
    "        decoder_optimizer: torch.optim.Optimizer,\n",
    "        criterion: callable,\n",
    "        seq_length: int,\n",
    "        hidden_size: int\n",
    "):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    end_seq = torch.tensor([102])\n",
    "    bsz = images.shape[0]\n",
    "    encoder_output: torch.Tensor = encoder(images) # [bsz, 256]\n",
    "    hidden = torch.zeros(1, bsz, hidden_size).to(device)\n",
    "    cell = torch.zeros(1, bsz, hidden_size).to(device)\n",
    "    hidden_state = (hidden, cell)\n",
    "    total_loss = 0\n",
    "    predicts = []\n",
    "    decoder_input = torch.tensor([101]).expand(hidden.shape[1]).unsqueeze(1).to(device)\n",
    "    for i in range(1, seq_length):\n",
    "        # print(\"Start\")\n",
    "        # print(encoder_output.shape)\n",
    "        # print(decoder_input.shape)\n",
    "        # print(hidden_state[0].shape)\n",
    "        decoder_output, hidden_state = decoder(encoder_output, decoder_input, hidden_state)\n",
    "        decoder_output: torch.Tensor\n",
    "        hidden_state: tuple[torch.Tensor, torch.Tensor]\n",
    "        # print(decoder_output.shape)\n",
    "        # print(hidden_state[0].shape)\n",
    "        loss: torch.Tensor = criterion(decoder_output.squeeze(1), captions[:, i])\n",
    "        loss.backward(retain_graph=True) # Acculumate grad\n",
    "        total_loss += loss.item()\n",
    "        predicted = decoder_output.argmax(dim=2)\n",
    "        # print(predicted.shape)\n",
    "        for j in range(predicted.shape[0]-1, -1, -1):\n",
    "            if predicted[j].item() == end_seq.item():\n",
    "                filter_mask = torch.zeros_like(hidden_state[0])\n",
    "                filter_mask[j] = False\n",
    "                hidden_state[0] = hidden_state[0][filter_mask]\n",
    "                hidden_state[1] = hidden_state[1][filter_mask]\n",
    "                filter_mask = torch.zeros_like(captions)\n",
    "                filter_mask[j] = False\n",
    "                captions = captions[filter_mask]\n",
    "                filter_mask = torch.zeros_like(encoder_output)\n",
    "                filter_mask[j] = False\n",
    "                encoder_output = encoder_output[filter_mask]\n",
    "        if encoder_output.shape[0] == 0:\n",
    "            break\n",
    "        decoder_input = captions[:, i].unsqueeze(1)\n",
    "    decoder_optimizer.step()\n",
    "    encoder_optimizer.step()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan | 1/118\n",
      "nan | 2/118\n",
      "319.59888 | 3/118\n",
      "319.06257 | 4/118\n",
      "317.68958 | 5/118\n",
      "316.02462 | 6/118\n",
      "310.33024 | 7/118\n",
      "304.15399 | 8/118\n",
      "299.97501 | 9/118\n",
      "290.47260 | 10/118\n",
      "282.54886 | 11/118\n",
      "275.94316 | 12/118\n",
      "271.06742 | 13/118\n",
      "265.59258 | 14/118\n",
      "261.00267 | 15/118\n",
      "256.44065 | 16/118\n",
      "251.41568 | 17/118\n",
      "247.18878 | 18/118\n",
      "241.80800 | 19/118\n",
      "237.06761 | 20/118\n",
      "231.75127 | 21/118\n",
      "227.21666 | 22/118\n",
      "222.62193 | 23/118\n",
      "217.90091 | 24/118\n",
      "212.78600 | 25/118\n",
      "208.03147 | 26/118\n",
      "203.09616 | 27/118\n",
      "198.04580 | 28/118\n",
      "193.55909 | 29/118\n",
      "188.89791 | 30/118\n",
      "183.91577 | 31/118\n",
      "179.36756 | 32/118\n",
      "174.40760 | 33/118\n",
      "169.84485 | 34/118\n",
      "165.42201 | 35/118\n",
      "160.90107 | 36/118\n",
      "156.90531 | 37/118\n",
      "152.87543 | 38/118\n",
      "148.69896 | 39/118\n",
      "144.85155 | 40/118\n",
      "141.03374 | 41/118\n",
      "137.75021 | 42/118\n",
      "134.26429 | 43/118\n",
      "130.65340 | 44/118\n",
      "128.16868 | 45/118\n",
      "124.79240 | 46/118\n",
      "122.57115 | 47/118\n",
      "119.82994 | 48/118\n",
      "117.34057 | 49/118\n",
      "115.77596 | 50/118\n",
      "113.39164 | 51/118\n",
      "112.13243 | 52/118\n",
      "110.22292 | 53/118\n",
      "108.74863 | 54/118\n",
      "107.09298 | 55/118\n",
      "106.13773 | 56/118\n",
      "104.76609 | 57/118\n",
      "103.30501 | 58/118\n",
      "102.66062 | 59/118\n",
      "101.20054 | 60/118\n",
      "101.67295 | 61/118\n",
      "100.50812 | 62/118\n",
      "99.67963 | 63/118\n",
      "99.79306 | 64/118\n",
      "98.82883 | 65/118\n",
      "98.92402 | 66/118\n",
      "97.50496 | 67/118\n",
      "98.36786 | 68/118\n",
      "97.15481 | 69/118\n",
      "97.06309 | 70/118\n",
      "96.63918 | 71/118\n",
      "96.58093 | 72/118\n",
      "96.89033 | 73/118\n",
      "96.52107 | 74/118\n",
      "96.11343 | 75/118\n",
      "95.47170 | 76/118\n",
      "95.55776 | 77/118\n",
      "95.58913 | 78/118\n",
      "95.46733 | 79/118\n",
      "94.95126 | 80/118\n",
      "95.06191 | 81/118\n",
      "95.42659 | 82/118\n",
      "95.06112 | 83/118\n",
      "95.35859 | 84/118\n",
      "94.74525 | 85/118\n",
      "95.09115 | 86/118\n",
      "94.76900 | 87/118\n",
      "94.99196 | 88/118\n",
      "94.37988 | 89/118\n",
      "94.00728 | 90/118\n",
      "94.51372 | 91/118\n",
      "94.26189 | 92/118\n",
      "94.48931 | 93/118\n",
      "94.32468 | 94/118\n",
      "94.40644 | 95/118\n",
      "93.75836 | 96/118\n",
      "94.10842 | 97/118\n",
      "94.53594 | 98/118\n",
      "93.49633 | 99/118\n",
      "93.61748 | 100/118\n",
      "93.94270 | 101/118\n",
      "93.38459 | 102/118\n",
      "93.86145 | 103/118\n",
      "93.32819 | 104/118\n",
      "93.73685 | 105/118\n",
      "93.73028 | 106/118\n",
      "93.52170 | 107/118\n",
      "93.65454 | 108/118\n",
      "93.02721 | 109/118\n",
      "93.58074 | 110/118\n",
      "93.99751 | 111/118\n",
      "92.71201 | 112/118\n",
      "93.41011 | 113/118\n",
      "93.35789 | 114/118\n",
      "93.20203 | 115/118\n",
      "94.09294 | 116/118\n",
      "93.36691 | 117/118\n",
      "92.59336 | 118/118\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderCNN()\n",
    "decoder = DecoderRNN(\n",
    "    embed_size=16,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,\n",
    "    input_size=256,\n",
    "    num_layers=1\n",
    ")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "lossf = nn.CrossEntropyLoss(ignore_index=0)\n",
    "i = 0\n",
    "total_loss = 0\n",
    "for images, captions in trainloader:\n",
    "    loss = train_single_batch(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        images=images,\n",
    "        captions=captions,\n",
    "        encoder_optimizer=encoder_optimizer,\n",
    "        decoder_optimizer=decoder_optimizer,\n",
    "        criterion=lossf,\n",
    "        seq_length=32,\n",
    "        hidden_size=128\n",
    "    )\n",
    "    total_loss += loss\n",
    "    i+=1\n",
    "    print(f\"{loss:.5f} | {i}/{len(trainloader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

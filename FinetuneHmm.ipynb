{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dùng tạm mô hình tự code để cho nó đúng đầu vào đầu ra đã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from pickle import dump, load\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import math\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "BATCH_SIZE = 64 # 3.2GB VRAM f32 (2.9 Dedicated + 0.2 Shared)\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((324, 324)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Dùng cho ImageNet\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "1000\n",
      "30000\n",
      "5000\n",
      "torch.Size([3, 324, 324]) torch.Size([46])\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "data_folder_path = \"Flickr8k/Flicker8k_Dataset\"\n",
    "train_dataset = util.Flickr8kDataset(\n",
    "    data_folder_path,\n",
    "    \"Data_bert/train_set_bert.pkl\",\n",
    "    image_transforms,\n",
    "    device=device\n",
    ")\n",
    "trainloader = train_dataset.get_dataloader(batch_size=BATCH_SIZE, num_workers=2, shuffle=False)\n",
    "test_dataset = util.Flickr8kDataset(\n",
    "    data_folder_path,\n",
    "    \"Data_bert/test_set_bert.pkl\",\n",
    "    image_transforms,\n",
    "    device=device\n",
    ")\n",
    "testloader = test_dataset.get_dataloader(batch_size=BATCH_SIZE, num_workers=2, shuffle=False)\n",
    "print(len(train_dataset)) # :)) sao có tận 30k ids ảnh trong train_set_bert.pkl, một ảnh có nhiều caption à ?\n",
    "print(len(test_dataset)) \n",
    "sample_image, sample_caption = test_dataset[0]\n",
    "print(sample_image.shape, sample_caption.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNNTest(nn.Module):\n",
    "    def __init__(self, output_size: int):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.CNN = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=3), # 324 * 324\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=3), # 108 * 108\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1), # 36 * 36\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3), # 12 * 12\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1), # 10 * 10\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1), # 8* 8 ?? 7 * 7\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, out_features=512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(512, output_size)\n",
    "        )\n",
    "    def forward(self, images: torch.Tensor):\n",
    "        return self.CNN(images)\n",
    "class DecoderRNNTest(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, hidden_size, input_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, hiden_state: tuple[torch.Tensor, torch.Tensor]): \n",
    "        # features : image_features : [bsz, embed]\n",
    "        # captions : [bsz, seq]\n",
    "        # hidden : [1, bsz, embed]\n",
    "        # print(\"Input dim\")\n",
    "        # print(features.shape)\n",
    "        # print(captions.shape)\n",
    "        # print(hiden_state[0].shape)\n",
    "        embeddings = self.embed(captions) # [bsz, seq, embed]\n",
    "        features = features.unsqueeze(1).expand(-1, embeddings.shape[1], -1) # [bsz, seq, embed]\n",
    "        combined = torch.cat((embeddings, features), dim=2) # [bsz, seq, embed*2]\n",
    "        # print(combined.shape, hiden_state[0].shape, hiden_state[1].shape)\n",
    "        output, hidden = self.lstm(combined, hiden_state) # [bsz, seq, hid]\n",
    "        output = self.relu(output)\n",
    "        output = self.linear(output) #[batch_size, seq_len, vocab_size]\n",
    "        # print(\"End\")\n",
    "\n",
    "        return output, hidden\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, output_size: int):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.inception_model = models.inception_v3(pretrained=True)\n",
    "        #self.inception_model.fc = torch.nn.Identity()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(1000, output_size)\n",
    "        for name, param in self.inception_model.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    def forward(self, images: torch.Tensor):\n",
    "        # print(\"encoder_model\")\n",
    "        features = self.inception_model(images) #[1, 2048]\n",
    "        if isinstance(features, tuple):  # Nếu là tuple\n",
    "            features = features[0] \n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "        features = self.fc(features)\n",
    "        # print(\"encoder_output.shape :\", features.shape)\n",
    "        return features #[1, 256]\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, hidden_size, input_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, features: torch.Tensor, captions: torch.Tensor, hiden_state: tuple[torch.Tensor, torch.Tensor]): \n",
    "        # features : image_features : [bsz, embed]\n",
    "        # captions : [bsz, seq]\n",
    "        # hidden : [1, bsz, embed]\n",
    "        # print(\"Input dim\")\n",
    "        # print(features.shape)\n",
    "        # print(captions.shape)\n",
    "        # print(hiden_state[0].shape)\n",
    "        embeddings = self.embed(captions) # [bsz, seq, embed]\n",
    "        features = features.unsqueeze(1).expand(-1, embeddings.shape[1], -1) # [bsz, seq, embed]\n",
    "        combined = torch.cat((embeddings, features), dim=2) # [bsz, seq, embed*2]\n",
    "        # print(combined.shape, hiden_state[0].shape, hiden_state[1].shape)\n",
    "        output, hidden = self.lstm(combined, hiden_state) # [bsz, seq, hid]\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output) #[batch_size, seq_len, vocab_size]\n",
    "        # print(\"End\")\n",
    "\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Test Encoder\n",
    "# encoder = EncoderCNN(256).to(device)\n",
    "# encoder.eval()\n",
    "# output_en = encoder(train_dataset[0][0].unsqueeze(0))\n",
    "# print(output_en.shape)\n",
    "# print(tokenizer.convert_ids_to_tokens(101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test decoder\n",
    "# vocab_size = tokenizer.vocab_size\n",
    "# embed_size = 16\n",
    "# num_layers = 1\n",
    "# hidden_size = 128\n",
    "# hiden = torch.zeros(1, 1, hidden_size).to(device)\n",
    "# cell = torch.zeros(1, 1, hidden_size).to(device)\n",
    "# hiden_state = (hiden, cell)\n",
    "# hiden_state[0].shape\n",
    "# decoder = DecoderRNN(\n",
    "#     embed_size=embed_size, \n",
    "#     vocab_size=vocab_size, \n",
    "#     hidden_size=hidden_size, \n",
    "#     input_size=256,\n",
    "#     num_layers=num_layers\n",
    "# ).to(device)\n",
    "# decoder.eval()\n",
    "# state = torch.tensor([101, 102, 103]).unsqueeze(0).to(device)\n",
    "# # print(output_en.shape)\n",
    "# # print(state.shape)\n",
    "# # print(hiden_state[0].shape, hiden_state[1].shape)\n",
    "# outputs, hidden = decoder(output_en, state, hiden_state)\n",
    "# # print(outputs.shape)\n",
    "# # print(hidden[0].shape)\n",
    "# # print(outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_batch(\n",
    "        encoder: nn.Module,\n",
    "        decoder: nn.Module,\n",
    "        images: torch.Tensor,\n",
    "        captions: torch.Tensor,\n",
    "        encoder_optimizer: torch.optim.Optimizer,\n",
    "        decoder_optimizer: torch.optim.Optimizer,\n",
    "        criterion: callable,\n",
    "        seq_length: int,\n",
    "        hidden_size: int\n",
    "):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    end_seq = torch.tensor([102])\n",
    "    bsz = images.shape[0]\n",
    "    # with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    encoder_output: torch.Tensor = encoder(images) # [bsz, 256]\n",
    "    hidden = torch.zeros(1, bsz, hidden_size).to(device)\n",
    "    cell = torch.zeros(1, bsz, hidden_size).to(device)\n",
    "    hidden_state = (hidden, cell)\n",
    "    total_loss = 0\n",
    "    predicts = []\n",
    "    decoder_input = torch.tensor([101]).expand(hidden.shape[1]).unsqueeze(1).to(device)\n",
    "    count = 0\n",
    "    for i in range(1, seq_length):\n",
    "        # print(\"Start\")\n",
    "        # print(encoder_output.shape)\n",
    "        # print(decoder_input.shape)\n",
    "        # print(hidden_state[0].shape)\n",
    "        # with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        decoder_output, hidden_state = decoder(encoder_output, decoder_input, hidden_state)\n",
    "        decoder_output: torch.Tensor\n",
    "        hidden_state: tuple[torch.Tensor, torch.Tensor]\n",
    "        # print(decoder_output.shape)\n",
    "        # print(hidden_state[0].shape)\n",
    "        loss: torch.Tensor = criterion(decoder_output.squeeze(1), captions[:, i])\n",
    "        loss.backward(retain_graph=True) # Accumulate grad\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "        predicted = decoder_output.argmax(dim=2)\n",
    "        # print(predicted.shape)\n",
    "        for j in range(predicted.shape[0]-1, -1, -1): # Drop data nếu nó ra end token\n",
    "            if predicted[j].item() == end_seq.item():\n",
    "                filter_mask = torch.zeros_like(hidden_state[0])\n",
    "                # print(filter_mask.shape)\n",
    "                filter_mask[:,j] = False\n",
    "                hidden = hidden_state[0][filter_mask.to(torch.bool)]\n",
    "                cell = hidden_state[1][filter_mask.to(torch.bool)]\n",
    "                hidden_state = (hidden, cell)\n",
    "                filter_mask = torch.zeros_like(captions)\n",
    "                filter_mask[j] = False\n",
    "                captions = captions[filter_mask.to(torch.bool)]\n",
    "                filter_mask = torch.zeros_like(encoder_output)\n",
    "                filter_mask[j] = False\n",
    "                encoder_output = encoder_output[filter_mask.to(torch.bool)]\n",
    "        if encoder_output.shape[0] == 0:\n",
    "            break\n",
    "        decoder_input = captions[:, i].unsqueeze(1)\n",
    "    decoder_optimizer.step()\n",
    "    encoder_optimizer.step()\n",
    "    return total_loss/count if count != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Anh\\.conda\\envs\\data\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan | 1/469\n",
      "nan | 2/469\n",
      "10.33457 | 3/469\n",
      "10.21784 | 4/469\n",
      "10.08265 | 5/469\n",
      "9.90005 | 6/469\n",
      "9.64418 | 7/469\n",
      "9.49579 | 8/469\n",
      "9.29390 | 9/469\n",
      "9.16287 | 10/469\n",
      "8.98156 | 11/469\n",
      "8.85970 | 12/469\n",
      "8.68585 | 13/469\n",
      "8.53561 | 14/469\n",
      "8.43100 | 15/469\n",
      "8.26295 | 16/469\n",
      "8.14614 | 17/469\n",
      "7.92569 | 18/469\n",
      "7.79030 | 19/469\n",
      "7.63027 | 20/469\n",
      "7.43530 | 21/469\n",
      "7.35094 | 22/469\n",
      "7.13353 | 23/469\n",
      "6.99972 | 24/469\n",
      "6.83039 | 25/469\n",
      "6.67528 | 26/469\n",
      "6.52067 | 27/469\n",
      "6.41031 | 28/469\n",
      "6.25714 | 29/469\n",
      "6.07256 | 30/469\n",
      "5.96062 | 31/469\n",
      "5.82471 | 32/469\n",
      "5.62772 | 33/469\n",
      "5.53043 | 34/469\n",
      "5.40219 | 35/469\n",
      "5.21612 | 36/469\n",
      "5.13587 | 37/469\n",
      "4.97039 | 38/469\n",
      "4.89724 | 39/469\n",
      "4.82156 | 40/469\n",
      "4.64891 | 41/469\n",
      "4.52030 | 42/469\n",
      "4.48322 | 43/469\n",
      "4.40249 | 44/469\n",
      "4.28216 | 45/469\n",
      "4.19617 | 46/469\n",
      "4.11442 | 47/469\n",
      "4.04193 | 48/469\n",
      "3.98858 | 49/469\n",
      "3.91588 | 50/469\n",
      "3.87367 | 51/469\n",
      "3.80503 | 52/469\n",
      "3.74158 | 53/469\n",
      "3.67746 | 54/469\n",
      "3.65070 | 55/469\n",
      "3.63548 | 56/469\n",
      "3.60014 | 57/469\n",
      "3.57552 | 58/469\n",
      "3.51101 | 59/469\n",
      "3.52677 | 60/469\n",
      "3.49428 | 61/469\n",
      "3.44840 | 62/469\n",
      "3.42496 | 63/469\n",
      "3.43216 | 64/469\n",
      "3.36257 | 65/469\n",
      "3.36896 | 66/469\n",
      "3.32613 | 67/469\n",
      "3.34319 | 68/469\n",
      "3.30870 | 69/469\n",
      "3.33957 | 70/469\n",
      "3.26919 | 71/469\n",
      "3.27841 | 72/469\n",
      "3.22951 | 73/469\n",
      "3.27683 | 74/469\n",
      "3.25891 | 75/469\n",
      "3.26469 | 76/469\n",
      "3.25218 | 77/469\n",
      "3.24321 | 78/469\n",
      "3.22133 | 79/469\n",
      "3.20533 | 80/469\n",
      "3.21714 | 81/469\n",
      "3.18535 | 82/469\n",
      "3.23046 | 83/469\n",
      "3.17677 | 84/469\n",
      "3.18067 | 85/469\n",
      "3.20103 | 86/469\n",
      "3.17972 | 87/469\n",
      "3.13888 | 88/469\n",
      "3.17227 | 89/469\n",
      "3.19211 | 90/469\n",
      "3.18753 | 91/469\n",
      "3.18005 | 92/469\n",
      "3.14958 | 93/469\n",
      "3.19058 | 94/469\n",
      "3.16660 | 95/469\n",
      "3.17964 | 96/469\n",
      "3.19126 | 97/469\n",
      "3.16184 | 98/469\n",
      "3.13179 | 99/469\n",
      "3.14705 | 100/469\n",
      "3.15080 | 101/469\n",
      "3.18898 | 102/469\n",
      "3.14515 | 103/469\n",
      "3.13195 | 104/469\n",
      "3.14313 | 105/469\n",
      "3.15876 | 106/469\n",
      "3.11647 | 107/469\n",
      "3.11393 | 108/469\n",
      "3.12368 | 109/469\n",
      "3.08892 | 110/469\n",
      "3.19252 | 111/469\n",
      "3.09790 | 112/469\n",
      "3.15021 | 113/469\n",
      "3.14103 | 114/469\n",
      "3.10749 | 115/469\n",
      "3.13595 | 116/469\n",
      "3.14048 | 117/469\n",
      "3.08392 | 118/469\n",
      "3.14049 | 119/469\n",
      "3.13593 | 120/469\n",
      "3.12482 | 121/469\n",
      "3.12317 | 122/469\n",
      "3.12878 | 123/469\n",
      "3.13627 | 124/469\n",
      "3.18390 | 125/469\n",
      "3.08887 | 126/469\n",
      "3.10573 | 127/469\n",
      "3.08181 | 128/469\n",
      "3.06630 | 129/469\n",
      "3.11418 | 130/469\n",
      "3.12242 | 131/469\n",
      "3.13967 | 132/469\n",
      "3.09554 | 133/469\n",
      "3.10433 | 134/469\n",
      "3.09957 | 135/469\n",
      "3.10683 | 136/469\n",
      "3.07606 | 137/469\n",
      "3.08729 | 138/469\n",
      "3.13972 | 139/469\n",
      "3.09213 | 140/469\n",
      "3.12991 | 141/469\n",
      "3.05401 | 142/469\n",
      "3.11923 | 143/469\n",
      "3.09759 | 144/469\n",
      "3.10772 | 145/469\n",
      "3.11385 | 146/469\n",
      "3.09406 | 147/469\n",
      "3.05057 | 148/469\n",
      "3.08219 | 149/469\n",
      "3.07765 | 150/469\n",
      "3.04240 | 151/469\n",
      "3.09449 | 152/469\n",
      "3.06478 | 153/469\n",
      "3.11911 | 154/469\n",
      "3.10284 | 155/469\n",
      "3.11487 | 156/469\n",
      "3.07249 | 157/469\n",
      "3.09154 | 158/469\n",
      "3.11282 | 159/469\n",
      "3.10940 | 160/469\n",
      "3.07591 | 161/469\n",
      "3.08154 | 162/469\n",
      "3.09075 | 163/469\n",
      "3.07499 | 164/469\n",
      "3.05365 | 165/469\n",
      "3.12942 | 166/469\n",
      "3.07824 | 167/469\n",
      "3.08125 | 168/469\n",
      "3.07467 | 169/469\n",
      "3.07616 | 170/469\n",
      "3.13344 | 171/469\n",
      "3.05142 | 172/469\n",
      "3.04573 | 173/469\n",
      "3.09144 | 174/469\n",
      "3.09899 | 175/469\n",
      "3.06787 | 176/469\n",
      "3.02126 | 177/469\n",
      "3.09189 | 178/469\n",
      "3.13926 | 179/469\n",
      "3.03853 | 180/469\n",
      "3.06493 | 181/469\n",
      "3.05192 | 182/469\n",
      "3.05407 | 183/469\n",
      "3.05344 | 184/469\n",
      "3.09852 | 185/469\n",
      "3.04333 | 186/469\n",
      "3.04765 | 187/469\n",
      "3.07980 | 188/469\n",
      "3.05154 | 189/469\n",
      "3.03322 | 190/469\n",
      "3.05001 | 191/469\n",
      "3.04903 | 192/469\n",
      "3.02767 | 193/469\n",
      "3.05450 | 194/469\n",
      "3.02668 | 195/469\n",
      "3.04117 | 196/469\n",
      "3.09345 | 197/469\n",
      "3.08403 | 198/469\n",
      "3.03923 | 199/469\n",
      "3.09492 | 200/469\n",
      "3.03616 | 201/469\n",
      "3.04393 | 202/469\n",
      "3.07226 | 203/469\n",
      "3.07244 | 204/469\n",
      "3.05873 | 205/469\n",
      "3.04905 | 206/469\n",
      "3.08101 | 207/469\n",
      "3.12446 | 208/469\n",
      "3.07027 | 209/469\n",
      "3.04678 | 210/469\n",
      "3.01610 | 211/469\n",
      "3.06451 | 212/469\n",
      "3.08223 | 213/469\n",
      "3.06028 | 214/469\n",
      "3.05575 | 215/469\n",
      "3.05364 | 216/469\n",
      "3.04302 | 217/469\n",
      "3.03560 | 218/469\n",
      "3.07065 | 219/469\n",
      "3.05564 | 220/469\n",
      "3.05793 | 221/469\n",
      "3.02394 | 222/469\n",
      "3.05284 | 223/469\n",
      "3.07028 | 224/469\n",
      "3.06606 | 225/469\n",
      "3.02942 | 226/469\n",
      "3.02949 | 227/469\n",
      "3.03320 | 228/469\n",
      "3.02179 | 229/469\n",
      "3.02758 | 230/469\n",
      "3.05683 | 231/469\n",
      "3.02034 | 232/469\n",
      "3.03169 | 233/469\n",
      "3.01890 | 234/469\n",
      "3.00873 | 235/469\n",
      "3.09322 | 236/469\n",
      "3.00977 | 237/469\n",
      "3.00433 | 238/469\n",
      "3.01685 | 239/469\n",
      "3.02226 | 240/469\n",
      "3.07094 | 241/469\n",
      "3.04794 | 242/469\n",
      "3.01690 | 243/469\n",
      "3.07484 | 244/469\n",
      "3.01967 | 245/469\n",
      "3.06262 | 246/469\n",
      "3.04050 | 247/469\n",
      "3.04294 | 248/469\n",
      "3.00986 | 249/469\n",
      "3.01273 | 250/469\n",
      "3.02025 | 251/469\n",
      "3.05124 | 252/469\n",
      "3.04954 | 253/469\n",
      "3.05988 | 254/469\n",
      "3.04329 | 255/469\n",
      "3.05346 | 256/469\n",
      "3.05559 | 257/469\n",
      "3.05253 | 258/469\n",
      "3.00364 | 259/469\n",
      "3.01089 | 260/469\n",
      "3.05002 | 261/469\n",
      "3.05854 | 262/469\n",
      "3.06671 | 263/469\n",
      "3.03780 | 264/469\n",
      "3.02307 | 265/469\n",
      "2.97980 | 266/469\n",
      "3.00963 | 267/469\n",
      "3.04012 | 268/469\n",
      "3.02271 | 269/469\n",
      "3.05076 | 270/469\n",
      "3.08251 | 271/469\n",
      "3.03931 | 272/469\n",
      "3.01303 | 273/469\n",
      "3.02397 | 274/469\n",
      "3.00685 | 275/469\n",
      "3.04189 | 276/469\n",
      "2.98174 | 277/469\n",
      "3.02083 | 278/469\n",
      "3.05007 | 279/469\n",
      "3.04823 | 280/469\n",
      "3.00842 | 281/469\n",
      "3.03176 | 282/469\n",
      "3.02954 | 283/469\n",
      "3.00421 | 284/469\n",
      "3.06958 | 285/469\n",
      "3.01271 | 286/469\n",
      "2.97513 | 287/469\n",
      "3.04356 | 288/469\n",
      "3.04269 | 289/469\n",
      "3.00337 | 290/469\n",
      "3.03792 | 291/469\n",
      "3.06965 | 292/469\n",
      "3.06093 | 293/469\n",
      "3.00225 | 294/469\n",
      "3.03879 | 295/469\n",
      "3.03240 | 296/469\n",
      "3.00877 | 297/469\n",
      "3.02581 | 298/469\n",
      "3.03621 | 299/469\n",
      "3.00025 | 300/469\n",
      "3.01085 | 301/469\n",
      "3.02755 | 302/469\n",
      "3.03401 | 303/469\n",
      "2.97373 | 304/469\n",
      "3.02533 | 305/469\n",
      "3.04535 | 306/469\n",
      "2.97883 | 307/469\n",
      "3.01714 | 308/469\n",
      "3.01174 | 309/469\n",
      "3.02374 | 310/469\n",
      "3.01346 | 311/469\n",
      "3.03390 | 312/469\n",
      "3.02375 | 313/469\n",
      "3.03238 | 314/469\n",
      "3.02393 | 315/469\n",
      "3.01902 | 316/469\n",
      "3.04296 | 317/469\n",
      "3.00209 | 318/469\n",
      "3.00288 | 319/469\n",
      "2.96865 | 320/469\n",
      "3.02791 | 321/469\n",
      "2.98137 | 322/469\n",
      "3.03540 | 323/469\n",
      "3.01240 | 324/469\n",
      "2.99847 | 325/469\n",
      "3.00086 | 326/469\n",
      "3.03152 | 327/469\n",
      "3.04308 | 328/469\n",
      "2.95583 | 329/469\n",
      "3.04646 | 330/469\n",
      "3.01911 | 331/469\n",
      "3.05577 | 332/469\n",
      "2.99947 | 333/469\n",
      "3.04072 | 334/469\n",
      "3.03266 | 335/469\n",
      "3.04421 | 336/469\n",
      "2.99486 | 337/469\n",
      "3.03545 | 338/469\n",
      "2.98191 | 339/469\n",
      "3.01534 | 340/469\n",
      "3.01532 | 341/469\n",
      "3.01940 | 342/469\n",
      "3.00507 | 343/469\n",
      "3.03546 | 344/469\n",
      "3.03693 | 345/469\n",
      "3.03731 | 346/469\n",
      "2.97947 | 347/469\n",
      "3.00267 | 348/469\n",
      "3.06417 | 349/469\n",
      "3.02077 | 350/469\n",
      "3.02352 | 351/469\n",
      "3.01593 | 352/469\n",
      "2.99009 | 353/469\n",
      "2.99472 | 354/469\n",
      "3.01240 | 355/469\n",
      "3.04132 | 356/469\n",
      "3.00112 | 357/469\n",
      "2.99360 | 358/469\n",
      "2.97824 | 359/469\n",
      "3.00276 | 360/469\n",
      "3.02718 | 361/469\n",
      "3.00265 | 362/469\n",
      "2.99488 | 363/469\n",
      "3.01003 | 364/469\n",
      "3.01402 | 365/469\n",
      "2.99912 | 366/469\n",
      "2.98043 | 367/469\n",
      "3.02733 | 368/469\n",
      "3.01063 | 369/469\n",
      "2.98933 | 370/469\n",
      "3.04059 | 371/469\n",
      "2.98394 | 372/469\n",
      "3.00647 | 373/469\n",
      "2.99584 | 374/469\n",
      "3.01708 | 375/469\n",
      "3.01539 | 376/469\n",
      "2.99770 | 377/469\n",
      "3.01778 | 378/469\n",
      "3.02031 | 379/469\n",
      "3.01832 | 380/469\n",
      "3.01083 | 381/469\n",
      "2.97456 | 382/469\n",
      "3.02450 | 383/469\n",
      "2.97273 | 384/469\n",
      "2.99906 | 385/469\n",
      "2.99348 | 386/469\n",
      "3.01670 | 387/469\n",
      "3.02821 | 388/469\n",
      "3.01743 | 389/469\n",
      "3.01110 | 390/469\n",
      "3.04371 | 391/469\n",
      "3.00995 | 392/469\n",
      "2.97852 | 393/469\n",
      "2.99876 | 394/469\n",
      "3.00717 | 395/469\n",
      "2.98016 | 396/469\n",
      "2.98696 | 397/469\n",
      "3.00028 | 398/469\n",
      "3.00309 | 399/469\n",
      "2.98244 | 400/469\n",
      "2.98483 | 401/469\n",
      "2.97939 | 402/469\n",
      "3.01327 | 403/469\n",
      "3.02075 | 404/469\n",
      "2.98316 | 405/469\n",
      "3.02242 | 406/469\n",
      "2.99542 | 407/469\n",
      "2.94850 | 408/469\n",
      "2.98453 | 409/469\n",
      "2.97918 | 410/469\n",
      "3.03497 | 411/469\n",
      "3.01589 | 412/469\n",
      "2.99072 | 413/469\n",
      "2.97631 | 414/469\n",
      "2.97657 | 415/469\n",
      "2.99633 | 416/469\n",
      "2.99946 | 417/469\n",
      "3.04507 | 418/469\n",
      "2.96177 | 419/469\n",
      "2.98912 | 420/469\n",
      "2.97362 | 421/469\n",
      "3.00409 | 422/469\n",
      "3.01196 | 423/469\n",
      "2.99407 | 424/469\n",
      "2.99338 | 425/469\n",
      "2.98809 | 426/469\n",
      "2.99862 | 427/469\n",
      "2.95760 | 428/469\n",
      "3.00602 | 429/469\n",
      "3.00811 | 430/469\n",
      "2.97079 | 431/469\n",
      "3.01270 | 432/469\n",
      "2.95838 | 433/469\n",
      "2.97702 | 434/469\n",
      "2.97078 | 435/469\n",
      "2.98550 | 436/469\n",
      "3.00180 | 437/469\n",
      "2.99154 | 438/469\n",
      "3.02011 | 439/469\n",
      "2.95585 | 440/469\n",
      "3.00620 | 441/469\n",
      "3.01342 | 442/469\n",
      "3.05805 | 443/469\n",
      "2.97268 | 444/469\n",
      "2.97137 | 445/469\n",
      "2.97324 | 446/469\n",
      "2.96875 | 447/469\n",
      "2.95982 | 448/469\n",
      "2.99605 | 449/469\n",
      "3.01329 | 450/469\n",
      "2.99375 | 451/469\n",
      "2.95321 | 452/469\n",
      "2.98216 | 453/469\n",
      "2.99187 | 454/469\n",
      "2.99161 | 455/469\n",
      "2.98477 | 456/469\n",
      "2.95646 | 457/469\n",
      "2.99143 | 458/469\n",
      "2.98099 | 459/469\n",
      "3.00390 | 460/469\n",
      "3.03778 | 461/469\n",
      "2.99127 | 462/469\n",
      "3.01247 | 463/469\n",
      "3.02516 | 464/469\n",
      "3.00666 | 465/469\n",
      "3.01011 | 466/469\n",
      "2.95675 | 467/469\n",
      "2.98100 | 468/469\n",
      "2.97439 | 469/469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [07:08<00:00, 428.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Test loss : 3.45287829012949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderCNN(\n",
    "    output_size=256\n",
    ")\n",
    "decoder = DecoderRNN(\n",
    "    embed_size=16,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,\n",
    "    input_size=256,\n",
    "    num_layers=1\n",
    ")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "lossf = nn.CrossEntropyLoss(ignore_index=0)\n",
    "num_epochs = 1\n",
    "# print(encoder)\n",
    "# print(decoder)\n",
    "for epoch in tqdm.trange(num_epochs):\n",
    "    i = 0\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    for images, captions in trainloader:\n",
    "        loss = train_single_batch(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            images=images,\n",
    "            captions=captions,\n",
    "            encoder_optimizer=encoder_optimizer,\n",
    "            decoder_optimizer=decoder_optimizer,\n",
    "            criterion=lossf,\n",
    "            seq_length=32,\n",
    "            hidden_size=128\n",
    "        )\n",
    "        total_loss += loss if not math.isnan(loss) else 0\n",
    "        count += 1 if not math.isnan(loss) else 0\n",
    "        i+=1\n",
    "        # print(\"Finish batch\")\n",
    "        print(f\"{loss:.5f} | {i}/{len(trainloader)}\")\n",
    "    print(f\"Epoch {epoch+1} | Test loss : {total_loss/count}\")\n",
    "    # random : log(1/30k) ~ 10.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037]\n",
      "tensor([ 101, 1037, 1048, 1045, 1056, 1056, 1048, 1041, 1043, 1045, 1054, 1048,\n",
      "        1039, 1051, 1058, 1041, 1054, 1041, 1040, 1045, 1050, 1052, 1037, 1045,\n",
      "        1050, 1056, 1055, 1045, 1056, 1055, 1045, 1050, 1042, 1054, 1051, 1050,\n",
      "        1056, 1051, 1042, 1037, 1052, 1037, 1045, 1050, 1056,  102],\n",
      "       device='cuda:0')\n",
      "[CLS] [CLS]\n",
      "a a\n",
      "a l\n",
      "a i\n",
      "a t\n",
      "a t\n",
      "a l\n",
      "a e\n",
      "a g\n",
      "a i\n",
      "a r\n",
      "a l\n",
      "a c\n",
      "a o\n",
      "a v\n",
      "a e\n",
      "a r\n",
      "a e\n",
      "a d\n",
      "a i\n",
      "a n\n",
      "a p\n",
      "a a\n",
      "a i\n",
      "a n\n",
      "a t\n",
      "a s\n",
      "a i\n",
      "a t\n",
      "a s\n",
      "a i\n",
      "a n\n"
     ]
    }
   ],
   "source": [
    "def test_sample(\n",
    "        encoder: nn.Module,\n",
    "        decoder: nn.Module,\n",
    "        dataset: Dataset,\n",
    "        index: int,\n",
    "        seq_length: int,\n",
    "        hidden_size: int\n",
    "):\n",
    "    images, captions = dataset[index]\n",
    "    images: torch.Tensor\n",
    "    captions: torch.Tensor\n",
    "    images = images.unsqueeze(0)\n",
    "    captions = captions.unsqueeze(0)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    end_seq = torch.tensor([102])\n",
    "    bsz = images.shape[0]\n",
    "    # with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    encoder_output: torch.Tensor = encoder(images) # [bsz, 256]\n",
    "    hidden = torch.zeros(1, bsz, hidden_size).to(device)\n",
    "    cell = torch.zeros(1, bsz, hidden_size).to(device)\n",
    "    hidden_state = (hidden, cell)\n",
    "    total_loss = 0\n",
    "    predicts = [[101] for _ in range(bsz)]\n",
    "    decoder_input = torch.tensor([101]).expand(hidden.shape[1]).unsqueeze(1).to(device)\n",
    "    count = 0\n",
    "    for i in range(1, seq_length):\n",
    "        decoder_output, hidden_state = decoder(encoder_output, decoder_input, hidden_state)\n",
    "        decoder_output: torch.Tensor\n",
    "        hidden_state: tuple[torch.Tensor, torch.Tensor]\n",
    "        count += 1\n",
    "        predicted = decoder_output.argmax(dim=2)\n",
    "        for j in range(predicted.shape[0]-1, -1, -1):\n",
    "            predicts[j].append(predicted[j].item())\n",
    "            if predicted[j].item() == end_seq.item():\n",
    "                filter_mask = torch.zeros_like(hidden_state[0])\n",
    "                # print(filter_mask.shape)\n",
    "                filter_mask[:,j] = False\n",
    "                hidden = hidden_state[0][filter_mask.to(torch.bool)]\n",
    "                cell = hidden_state[1][filter_mask.to(torch.bool)]\n",
    "                hidden_state = (hidden, cell)\n",
    "                filter_mask = torch.zeros_like(captions)\n",
    "                filter_mask[j] = False\n",
    "                captions = captions[filter_mask.to(torch.bool)]\n",
    "                filter_mask = torch.zeros_like(encoder_output)\n",
    "                filter_mask[j] = False\n",
    "                encoder_output = encoder_output[filter_mask.to(torch.bool)]\n",
    "        if encoder_output.shape[0] == 0:\n",
    "            break\n",
    "        decoder_input = captions[:, i].unsqueeze(1)\n",
    "    return predicts, captions\n",
    "predicts, captions = test_sample(encoder, decoder, train_dataset, 10, 32, 128)\n",
    "# print(predicts, captions)\n",
    "predicts = predicts[0]\n",
    "captions = captions[0]\n",
    "print(predicts)\n",
    "print(captions)\n",
    "detokenize = tokenizer.convert_ids_to_tokens\n",
    "for i in range(min(len(predicts), captions.shape[0])):\n",
    "    print(detokenize(predicts[i]), detokenize(captions[i].item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

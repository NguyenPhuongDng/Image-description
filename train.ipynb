{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45820591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepare_data import create_flickr_dataloaders\n",
    "from encoder import ImageEncoder\n",
    "from decoder import CaptionDecoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65535e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(encoder, decoder, image_tensor, vocab_info, device, max_length=20):\n",
    "    \"\"\"Generate caption for a single image\"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Extract image features\n",
    "        image_features = encoder(image_tensor.unsqueeze(0).to(device))\n",
    "        \n",
    "        # Start with START token\n",
    "        caption = [vocab_info['start_token']]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            caption_tensor = torch.tensor([caption]).to(device)\n",
    "            outputs = decoder(caption_tensor, image_features)\n",
    "            \n",
    "            # Get next word\n",
    "            next_word = outputs[0, -1, :].argmax().item()\n",
    "            caption.append(next_word)\n",
    "            \n",
    "            # Stop if END token\n",
    "            if next_word == vocab_info['end_token']:\n",
    "                break\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657c3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_caption(caption_ids):\n",
    "    \"\"\"Convert caption IDs back to text\"\"\"\n",
    "    with open('Processed Data/decode_vocab.json', 'r') as f:\n",
    "        decode_vocab = json.load(f)\n",
    "    \n",
    "    words = []\n",
    "    for idx in caption_ids:\n",
    "        word = decode_vocab[str(idx)]\n",
    "        if word not in ['<START>', '<END>', '<PAD>']:\n",
    "            words.append(word)\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c545de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_prediction(encoder, decoder, test_loader, vocab_info, device, epoch):\n",
    "    \"\"\"Plot a sample image with predicted caption from TEST SET\"\"\"\n",
    "    import random\n",
    "    \n",
    "    # Set seed based on epoch ƒë·ªÉ reproducible nh∆∞ng kh√°c nhau\n",
    "    random.seed(epoch + 42)\n",
    "    \n",
    "    # Random pick m·ªôt batch t·ª´ test set\n",
    "    batch_idx = random.randint(0, len(test_loader) - 1)\n",
    "    \n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i == batch_idx:\n",
    "            images = batch['images']\n",
    "            image_ids = batch['image_ids']\n",
    "            \n",
    "            # Random pick m·ªôt sample trong batch\n",
    "            local_idx = random.randint(0, len(images) - 1)\n",
    "            \n",
    "            sample_image = images[local_idx]\n",
    "            sample_id = image_ids[local_idx]\n",
    "            \n",
    "            # Generate caption\n",
    "            predicted_caption = generate_caption(encoder, decoder, sample_image, vocab_info, device)\n",
    "            predicted_text = decode_caption(predicted_caption)\n",
    "            \n",
    "            # Load and display image\n",
    "            img_path = f\"Flickr8k/Flicker8k_Dataset/{sample_id}\"\n",
    "            image_pil = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(image_pil)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Image: {sample_id}\\nPredicted: {predicted_text}', \n",
    "                     fontsize=12, wrap=True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"Epoch {epoch} - TEST Image: {sample_id}\")\n",
    "            print(f\"Predicted Caption: {predicted_text}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d234d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(encoder, decoder, val_loader, criterion, device, vocab_info):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['images'].to(device)\n",
    "            captions = batch['captions'].to(device)\n",
    "            \n",
    "            image_features = encoder(images)\n",
    "            outputs = decoder(captions[:, :-1], image_features)\n",
    "            targets = captions[:, 1:]\n",
    "            \n",
    "            loss = criterion(outputs.reshape(-1, vocab_info['vocab_size']), \n",
    "                           targets.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58174895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlickrDataset train loaded:\n",
      "  Images: 6000\n",
      "  Image-caption pairs: 30000\n",
      "  Vocab size: 2549\n",
      "  Max length: 15\n",
      "FlickrDataset val loaded:\n",
      "  Images: 1000\n",
      "  Image-caption pairs: 5000\n",
      "  Vocab size: 2549\n",
      "  Max length: 15\n",
      "FlickrDataset test loaded:\n",
      "  Images: 1000\n",
      "  Image-caption pairs: 5000\n",
      "  Vocab size: 2549\n",
      "  Max length: 15\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, vocab_info = create_flickr_dataloaders(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader, vocab_info, num_epochs=10):\n",
    "    \n",
    "    # Initialize models\n",
    "    encoder = ImageEncoder(embed_dim=512)\n",
    "    decoder = CaptionDecoder(vocab_info['vocab_size'], embed_dim=512)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab_info['pad_token'])\n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
    "                          lr=1e-4, weight_decay=1e-3)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            images = batch['images'].to(device)\n",
    "            captions = batch['captions'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            image_features = encoder(images)\n",
    "            outputs = decoder(captions[:, :-1], image_features)\n",
    "            \n",
    "            # Calculate loss\n",
    "            targets = captions[:, 1:]\n",
    "            loss = criterion(outputs.reshape(-1, vocab_info['vocab_size']), \n",
    "                           targets.reshape(-1))\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 200 == 0:\n",
    "                print(f'Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = validate(encoder, decoder, val_loader, criterion, device, vocab_info)\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Plot sample prediction FROM TEST SET\n",
    "        plot_sample_prediction(encoder, decoder, test_loader, vocab_info, device, epoch)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'encoder_state': encoder.state_dict(),\n",
    "            'decoder_state': decoder.state_dict(),\n",
    "        }, f'checkpoint_epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20622155",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(train_loader, val_loader, vocab_info, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d539fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(encoder, decoder, test_loader, vocab_info, device):\n",
    "    \"\"\"Comprehensive evaluation on test set\"\"\"\n",
    "    from collections import Counter\n",
    "    import numpy as np\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    print(\"üîç Evaluating model on test set...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            images = batch['images'].to(device)\n",
    "            captions = batch['captions'].to(device)\n",
    "            image_ids = batch['image_ids']\n",
    "            \n",
    "            batch_size = len(images)\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            # Generate predictions for each image\n",
    "            for i in range(batch_size):\n",
    "                # Generate caption\n",
    "                predicted_caption = generate_caption(encoder, decoder, images[i], vocab_info, device)\n",
    "                predicted_text = decode_caption(predicted_caption)\n",
    "                \n",
    "                # Get target caption (first caption for this image)\n",
    "                target_caption = captions[i]\n",
    "                target_text = decode_caption(target_caption.cpu().tolist())\n",
    "                \n",
    "                all_predictions.append(predicted_text)\n",
    "                all_targets.append(target_text)\n",
    "                \n",
    "                # Simple accuracy: check if any target word appears in prediction\n",
    "                pred_words = set(predicted_text.lower().split())\n",
    "                target_words = set(target_text.lower().split())\n",
    "                if len(pred_words.intersection(target_words)) > 0:\n",
    "                    correct_predictions += 1\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Processed {batch_idx * batch_size} / {len(test_loader.dataset)} samples\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    \n",
    "    # Word frequency analysis\n",
    "    pred_words = []\n",
    "    target_words = []\n",
    "    for pred, target in zip(all_predictions, all_targets):\n",
    "        pred_words.extend(pred.lower().split())\n",
    "        target_words.extend(target.lower().split())\n",
    "    \n",
    "    pred_counter = Counter(pred_words)\n",
    "    target_counter = Counter(target_words)\n",
    "    \n",
    "    print(\"\\nüìä EVALUATION RESULTS:\")\n",
    "    print(f\"Total test samples: {total_samples}\")\n",
    "    print(f\"Word overlap accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Average prediction length: {np.mean([len(p.split()) for p in all_predictions]):.1f} words\")\n",
    "    print(f\"Average target length: {np.mean([len(t.split()) for t in all_targets]):.1f} words\")\n",
    "    \n",
    "    print(f\"\\nüî§ Most common predicted words:\")\n",
    "    for word, count in pred_counter.most_common(10):\n",
    "        print(f\"  {word}: {count}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Most common target words:\")\n",
    "    for word, count in target_counter.most_common(10):\n",
    "        print(f\"  {word}: {count}\")\n",
    "    \n",
    "    # Show sample predictions\n",
    "    print(f\"\\nüìù SAMPLE PREDICTIONS:\")\n",
    "    for i in range(min(10, len(all_predictions))):\n",
    "        print(f\"Target:    {all_targets[i]}\")\n",
    "        print(f\"Predicted: {all_predictions[i]}\")\n",
    "        print(\"---\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'total_samples': total_samples,\n",
    "        'predictions': all_predictions,\n",
    "        'targets': all_targets,\n",
    "        'pred_word_freq': pred_counter,\n",
    "        'target_word_freq': target_counter\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50a4b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(predictions, targets):\n",
    "    \"\"\"Calculate BLEU score (simplified version)\"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    total_score = 0\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        pred_words = pred.lower().split()\n",
    "        target_words = target.lower().split()\n",
    "        \n",
    "        if len(pred_words) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 1-gram precision\n",
    "        pred_counter = Counter(pred_words)\n",
    "        target_counter = Counter(target_words)\n",
    "        \n",
    "        overlap = 0\n",
    "        for word in pred_counter:\n",
    "            overlap += min(pred_counter[word], target_counter.get(word, 0))\n",
    "        \n",
    "        precision = overlap / len(pred_words) if len(pred_words) > 0 else 0\n",
    "        total_score += precision\n",
    "    \n",
    "    return total_score / len(predictions) if len(predictions) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac003b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model v√† test\n",
    "# Load best checkpoint\n",
    "checkpoint = torch.load('Models/checkpoint_epoch_9.pth', map_location=torch.device('cpu')) \n",
    "\n",
    "# Initialize models\n",
    "encoder = ImageEncoder(embed_dim=512)\n",
    "decoder = CaptionDecoder(vocab_info['vocab_size'], embed_dim=512)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Load weights\n",
    "encoder.load_state_dict(checkpoint['encoder_state'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state'])\n",
    "\n",
    "print(\"üéØ Model loaded! Starting evaluation...\")\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_model(encoder, decoder, test_loader, vocab_info, device)\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu = calculate_bleu_score(results['predictions'], results['targets'])\n",
    "print(f\"\\nüéØ BLEU Score: {bleu:.3f}\")\n",
    "\n",
    "# Diversity analysis\n",
    "unique_predictions = len(set(results['predictions']))\n",
    "print(f\"\\nüîç DIVERSITY ANALYSIS:\")\n",
    "print(f\"Unique predictions: {unique_predictions} / {results['total_samples']} ({unique_predictions/results['total_samples']:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
